==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\all_evaluation_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\all_evaluation_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\auto_gt.py ==== 
# src/core/evaluation/auto_gt.py
from __future__ import annotations
import re
from typing import Dict, List, Any
from sentence_transformers import SentenceTransformer, util


# Global citation pattern
_CIT_PATTERN = re.compile(r"\[(\d+)\]")


class AutoGroundTruth:
    """
    Automatic graded relevance labelling for retrieval evaluation.
    Uses same embedding model as your retrieval stack for consistency.
    """

    def __init__(self, model_name: str = "multi-qa-mpnet-base-dot-v1",
                 high_thr: float = 0.30, mid_thr: float = 0.15, low_thr: float = 0.07):
        self.model = SentenceTransformer(model_name)
        self.high_thr = high_thr
        self.mid_thr = mid_thr
        self.low_thr = low_thr

    def _extract_citations(self, output: str) -> List[int]:
        if not output:
            return []
        return [int(m.group(1)) for m in _CIT_PATTERN.finditer(output)]

    def build(self, answer: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, int]:
        if not answer or not retrieved_chunks:
            return {}

        # Encode once
        ans_emb = self.model.encode([answer], normalize_embeddings=True)

        labels: Dict[str, int] = {}
        cited = set(self._extract_citations(answer))

        for rank, ch in enumerate(retrieved_chunks, start=1):
            cid = ch.get("id") or f"auto::{rank}"
            text = ch.get("text", "") or ""
            chunk_emb = self.model.encode([text], normalize_embeddings=True)
            sim = float(util.cos_sim(ans_emb, chunk_emb)[0][0])

            # Citations count as strong relevance signals
            cited_here = int(ch.get("rank", rank)) in cited

            # graded relevance
            if cited_here and sim >= self.high_thr:
                rel = 3
            elif sim >= self.high_thr:
                rel = 2
            elif sim >= self.mid_thr:
                rel = 1
            elif sim >= self.low_thr:
                rel = 0
            else:
                rel = 0

            labels[cid] = int(rel)

        return labels

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\convergence_plotter.py ==== 
from __future__ import annotations
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List
from src.core.evaluation.plot_style import apply_scientific_style, annotate_sample_info

class ConvergencePlotter:
    """Aggregates summary.json files across n-stages and visualizes mean ±95% CI convergence."""

    def __init__(self, charts_dir: str = "data/eval_charts"):
        self.charts_dir = Path(charts_dir)
        apply_scientific_style()

    def _load_stage_summaries(self) -> List[dict]:
        """Collect all summary_n*.json files (one per n-stage)."""
        summaries = []
        for fp in sorted(self.charts_dir.glob("summary_n*.json")):
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))
                data["n"] = int("".join([c for c in fp.stem if c.isdigit()]))
                summaries.append(data)
            except Exception:
                continue
        return sorted(summaries, key=lambda x: x["n"])

    def plot(self) -> None:
        """Plot mean ±95% CI vs n for NDCG@k and Faithfulness."""
        data = self._load_stage_summaries()
        if not data:
            print("No stage summaries found in charts directory.")
            return

        ns = np.array([d["n"] for d in data])
        nd_mean = np.array([d["ndcg@k_mean"] for d in data])
        nd_lo = np.array([d["ndcg@k_ci95_lo"] for d in data])
        nd_hi = np.array([d["ndcg@k_ci95_hi"] for d in data])

        fa_mean = np.array([d["faith_mean"] for d in data])
        fa_lo = np.array([d["faith_ci95_lo"] for d in data])
        fa_hi = np.array([d["faith_ci95_hi"] for d in data])

        fig, ax = plt.subplots(figsize=(6.5, 4.5))

        # NDCG@k line
        ax.plot(ns, nd_mean, "-o", color="#1b9e77", label="NDCG@k mean")
        ax.fill_between(ns, nd_lo, nd_hi, color="#1b9e77", alpha=0.2)

        # Faithfulness line
        ax.plot(ns, fa_mean, "-o", color="#d95f02", label="Faithfulness mean")
        ax.fill_between(ns, fa_lo, fa_hi, color="#d95f02", alpha=0.2)

        ax.set_xlabel("Sample size n")
        ax.set_ylabel("Metric value")
        ax.set_title("Convergence of Evaluation Metrics (mean ±95% CI)")
        ax.set_ylim(0, 1.05)
        ax.legend(frameon=False)
        annotate_sample_info(ax, n=len(ns))

        fig.tight_layout()
        for ext in ("png", "svg"):
            fig.savefig(self.charts_dir / f"convergence_plot.{ext}", dpi=150, bbox_inches="tight")
        plt.close(fig)
        print(f"Convergence plot saved to {self.charts_dir}/convergence_plot.*")

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_orchestrator.py ==== 
# src/core/evaluation/evaluation_orchestrator.py
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

from src.core.evaluation.interfaces.i_metric import IMetric
from src.core.evaluation.metrics.ndcg_metric import NDCGMetric
from src.core.evaluation.metrics.faithfulness_metric import FaithfulnessMetric
from src.core.evaluation.ground_truth_builder import GroundTruthBuilder
from src.core.evaluation.utils import make_chunk_id

logger = logging.getLogger("EvaluationOrchestrator")


class EvaluationOrchestrator:
    """
    Fully deterministic evaluation orchestrator.

    Important:
    - The caller is responsible for giving a model-specific eval_dir, e.g. data/eval_logs_phi3_4b
    - This class writes *_evaluation.json + evaluation_summary.json into that directory.
    """

    def __init__(
        self,
        base_output_dir: str = "data/eval_logs",
        model_name: str = "default",
        k: int = 10
    ):
        # Use the directory exactly as provided by the caller
        self.model_name = model_name
        self.out = Path(base_output_dir)
        self.out.mkdir(parents=True, exist_ok=True)

        self.k = k

        # Core evaluation metrics
        self.metrics: Dict[str, IMetric] = {
            "ndcg@k": NDCGMetric(k=k),
            "faithfulness": FaithfulnessMetric(),
        }

        # Semantic ground truth builder
        self.gt_builder = GroundTruthBuilder()

        logger.info(
            f"Evaluation orchestrator ready | k={self.k} | out={self.out} | model={self.model_name}"
        )

    # ------------------------------------------------------------------
    def _ensure_chunk_ids(self, items: List[Dict[str, Any]]) -> None:
        # Ensure every retrieved chunk has a stable, unique ID
        for ch in items:
            if not ch.get("id"):
                ch["id"] = make_chunk_id(ch)

    # ------------------------------------------------------------------
    def _safe_id(self, s: str | None) -> str:
        # Generate a filesystem-safe identifier for query strings
        if not s:
            return "query"
        return "".join(ch if ch.isalnum() or ch in "-_" else "_" for ch in s)[:80] or "query"

    # ------------------------------------------------------------------
    def _safe_year(self, meta: Dict[str, Any]) -> Optional[int]:
        # Extract a plausible publication year
        y = meta.get("year")
        try:
            yi = int(str(y))
            if 1900 <= yi <= 2100:
                return yi
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _dominant_decade(self, chunks: List[Dict[str, Any]]) -> Tuple[Optional[int], Dict[str, int]]:
        # Compute the dominant decade distribution over retrieved chunks
        counts: Dict[str, int] = {}
        for ch in chunks:
            y = self._safe_year(ch.get("metadata", {}) or {})
            d = f"{(y // 10) * 10}s" if y else "unknown"
            counts[d] = counts.get(d, 0) + 1

        if not counts:
            return None, {}

        dom_dec_str = sorted(counts.items(), key=lambda kv: (-kv[1], kv[0]))[0][0]

        try:
            dom = int(dom_dec_str[:-1])
        except Exception:
            dom = None

        return dom, counts

    # ------------------------------------------------------------------
    def _parse_citation_map_from_prompt(self, prompt_text: str) -> Dict[int, str]:
        # Parse the [n] → filename mapping from the prompt preamble
        if not prompt_text:
            return {}

        import re
        citation_map: Dict[int, str] = {}
        pattern = re.compile(
            r"^\[(\d+)\]\s+(.+?)\s+\((?:\d{4}|n/a)\)\s*$",
            re.MULTILINE
        )

        for m in pattern.finditer(prompt_text):
            try:
                idx = int(m.group(1))
                fname = m.group(2).strip()
                citation_map[idx] = fname
            except Exception:
                continue

        return citation_map

    # ------------------------------------------------------------------
    def _citation_hit_rate(
        self,
        model_output: str,
        retrieved_chunks: List[Dict[str, Any]],
        citation_map: Dict[int, str]
    ) -> float:
        # Ratio of citations in the answer that actually point to retrieved sources
        if not model_output or not citation_map:
            return 0.0

        import re
        nums = [int(x) for x in re.findall(r"\[(\d+)\]", model_output)]
        if not nums:
            return 0.0

        retrieved_sources = {
            (ch.get("metadata", {}) or {}).get("source_file", "").strip().lower()
            for ch in retrieved_chunks
        }

        hits = 0
        for n in nums:
            fname = (citation_map.get(n) or "").strip().lower()
            if fname and fname in retrieved_sources:
                hits += 1

        return hits / len(nums)

    # ------------------------------------------------------------------
    def evaluate_single(
        self,
        query: str,
        retrieved_chunks: List[Dict[str, Any]],
        model_output: str,
        prompt_text: str | None = None
    ) -> Dict[str, float]:
        # Evaluate a single query-answer pair
        self._ensure_chunk_ids(retrieved_chunks)

        # Semantic ground truth for intrinsic metric
        gt_map = self.gt_builder.build(query, retrieved_chunks)
        relevance_scores = [
            int(gt_map.get(ch["id"], ch.get("relevance", 0)))
            for ch in retrieved_chunks
        ]

        ndcg_val = self.metrics["ndcg@k"].compute(relevance_scores=relevance_scores)
        faith_val = self.metrics["faithfulness"].compute(
            context_chunks=[c.get("text", "") for c in retrieved_chunks],
            answer=model_output,
        )

        dom_dec, dec_counts = self._dominant_decade(retrieved_chunks)
        cit_map = self._parse_citation_map_from_prompt(prompt_text or "")
        cit_hit = self._citation_hit_rate(model_output, retrieved_chunks, cit_map)

        qid = self._safe_id(query)
        result = {
            "query_id": qid,
            "ndcg@k": float(ndcg_val),
            "faithfulness": float(faith_val),
            "dominant_decade": int(dom_dec) if dom_dec is not None else None,
            "decade_counts": dec_counts,
            "citation_hit_rate": float(cit_hit),
            "model_name": self.model_name,
        }

        out_file = self.out / f"{qid}_evaluation.json"
        out_file.write_text(json.dumps(result, indent=2), encoding="utf-8")
        logger.info(f"Evaluation completed → {out_file}")

        return result

    # ------------------------------------------------------------------
    def evaluate_batch_from_logs(
        self,
        logs_dir: str = "data/logs",
        pattern: str = "llm_*.json"
    ) -> Dict[str, float]:
        """
        Batch-evaluate all LLM logs in logs_dir.

        Important:
        - logs_dir should usually be something like data/logs_phi3_4b
        - Only files matching pattern (default: llm_*.json) are evaluated
        - Files like llm_input_*.json are intentionally ignored
        """

        logs_path = Path(logs_dir)

        # Fallback: if the exact directory does not exist, try suffix with model_name
        if not logs_path.exists():
            alt = logs_path.parent / f"{logs_path.name}_{self.model_name}"
            if alt.exists():
                logs_path = alt
                logger.warning(f"Using fallback logs directory: {logs_path}")
            else:
                logger.error(
                    f"No log directory found for model '{self.model_name}': "
                    f"{logs_path} or {alt}"
                )
                return {
                    "model_name": self.model_name,
                    "files": 0,
                    "evaluated_files": 0,
                    "mean_ndcg@k": 0.0,
                    "mean_faithfulness": 0.0,
                }

        files = sorted(logs_path.glob(pattern))
        if not files:
            logger.error(f"No log files matching '{pattern}' in {logs_path}")
            return {
                "model_name": self.model_name,
                "files": 0,
                "evaluated_files": 0,
                "mean_ndcg@k": 0.0,
                "mean_faithfulness": 0.0,
            }

        nd_vals: List[float] = []
        fa_vals: List[float] = []
        evaluated = 0

        for fp in files:
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))

                query = (
                    data.get("query")
                    or data.get("user_query")
                    or data.get("prompt")
                    or data.get("query_refined")
                    or ""
                )

                model_output = data.get("model_output") or data.get("answer") or ""
                retrieved = data.get("retrieved_chunks") or data.get("context_snippets") or []
                prompt_text = data.get("prompt_final_to_llm") or ""

                for rank, ch in enumerate(retrieved, start=1):
                    ch.setdefault("rank", rank)
                    ch.setdefault("final_score", ch.get("score", 0.0))
                    if "text" not in ch and "snippet" in ch:
                        ch["text"] = ch["snippet"]

                if not query or not retrieved:
                    logger.warning(f"Skipped incomplete log: {fp.name}")
                    continue

                res = self.evaluate_single(query, retrieved, model_output, prompt_text)
                nd_vals.append(float(res["ndcg@k"]))
                fa_vals.append(float(res["faithfulness"]))
                evaluated += 1

            except Exception as e:
                err_path = self.out / f"{fp.stem}_eval_error.json"
                err_path.write_text(json.dumps({"error": str(e)}, indent=2), encoding="utf-8")
                logger.error(f"Evaluation failed for {fp.name}: {e}")

        mean_nd = float(sum(nd_vals) / len(nd_vals)) if nd_vals else 0.0
        mean_fa = float(sum(fa_vals) / len(fa_vals)) if fa_vals else 0.0

        summary = {
            "model_name": self.model_name,
            "files": len(files),
            "evaluated_files": evaluated,
            "mean_ndcg@k": mean_nd,
            "mean_faithfulness": mean_fa,
        }

        (self.out / "evaluation_summary.json").write_text(
            json.dumps(summary, indent=2),
            encoding="utf-8"
        )

        logger.info(
            f"Batch evaluation completed for model={self.model_name} | "
            f"files={len(files)} | evaluated={evaluated}"
        )
        return summary

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_table_exporter.py ==== 
from __future__ import annotations
import json
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List

class EvaluationTableExporter:
    """Exports evaluation summary statistics as LaTeX, CSV, and Markdown tables."""

    def __init__(self, charts_dir: str = "data/eval_charts"):
        self.charts_dir = Path(charts_dir)
        self.summary_path = self.charts_dir / "summary.json"
        self.out_tex = self.charts_dir / "evaluation_table.tex"
        self.out_csv = self.charts_dir / "evaluation_table.csv"
        self.out_md  = self.charts_dir / "evaluation_table.md"

    # ------------------------------------------------------------------
    def _load_summary(self) -> Dict[str, Any]:
        if not self.summary_path.exists():
            raise FileNotFoundError(f"Missing summary.json in {self.charts_dir}")
        return json.loads(self.summary_path.read_text(encoding="utf-8"))

    # ------------------------------------------------------------------
    def _format_value(self, mean: float, lo: float, hi: float, digits: int = 3) -> str:
        """Format mean ± CI string."""
        return f"{mean:.{digits}f} ± {((hi - lo) / 2):.{digits}f}"

    # ------------------------------------------------------------------
    def export(self) -> Dict[str, Any]:
        """Generate LaTeX/CSV/MD tables from summary.json."""
        data = self._load_summary()
        if not data or "files" not in data:
            raise ValueError("Invalid or incomplete summary.json")

        rows: List[Dict[str, Any]] = [
            {
                "Metric": "NDCG@k",
                "Mean ± CI": self._format_value(data["ndcg@k_mean"],
                                                data["ndcg@k_ci95_lo"],
                                                data["ndcg@k_ci95_hi"]),
                "Median": f"{data['ndcg@k_median']:.3f}",
                "Std": f"{data['ndcg@k_std']:.3f}"
            },
            {
                "Metric": "Faithfulness",
                "Mean ± CI": self._format_value(data["faith_mean"],
                                                data["faith_ci95_lo"],
                                                data["faith_ci95_hi"]),
                "Median": f"{data['faith_median']:.3f}",
                "Std": f"{data['faith_std']:.3f}"
            }
        ]

        df = pd.DataFrame(rows)
        # CSV
        df.to_csv(self.out_csv, index=False, encoding="utf-8")

        # Markdown
        md_lines = ["| Metric | Mean ± CI | Median | Std |",
                    "|:--|--:|--:|--:|"]
        for r in rows:
            md_lines.append(f"| {r['Metric']} | {r['Mean ± CI']} | {r['Median']} | {r['Std']} |")
        self.out_md.write_text("\n".join(md_lines), encoding="utf-8")

        # LaTeX
        tex = [
            "\\begin{table}[h]",
            "\\centering",
            "\\caption{Evaluation metrics with 95\\% confidence intervals (bootstrap).}",
            "\\label{tab:evaluation_results}",
            "\\begin{tabular}{lccc}",
            "\\toprule",
            "Metric & Mean $\\pm$ CI & Median & Std \\\\",
            "\\midrule",
        ]
        for r in rows:
            tex.append(f"{r['Metric']} & {r['Mean ± CI']} & {r['Median']} & {r['Std']} \\\\")
        tex.extend([
            "\\bottomrule",
            "\\end{tabular}",
            "\\end{table}"
        ])
        self.out_tex.write_text("\n".join(tex), encoding="utf-8")

        return {
            "csv": str(self.out_csv),
            "md": str(self.out_md),
            "tex": str(self.out_tex),
            "rows": len(rows)
        }

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_visualizer.py ==== 
# src/core/evaluation/evaluation_visualizer.py
from __future__ import annotations
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import spearmanr  # for rank-based correlation

from src.core.evaluation.plot_style import (
    apply_scientific_style,
)

# TUM/Harvard palette plus traffic-light bands
PRIMARY_COLOR = "#003359"      # deep blue (NDCG)
SECONDARY_COLOR = "#CC0000"    # crimson (Faithfulness line plots)
ACCENT_COLOR = "#FFB300"       # warm accent for scatter

FAITH_GOOD_COLOR = "#1a9850"   # green for high faithfulness
FAITH_MED_COLOR = "#fee08b"    # yellow for medium faithfulness
FAITH_LOW_COLOR = "#d73027"    # red for low faithfulness


@dataclass
class VizConfig:
    logs_dir: str = "data/eval_logs"
    out_dir: str = "data/eval_charts"
    pattern: str = "*_evaluation.json"
    bootstrap_iters: int = 2000
    random_seed: int = 42
    iqr_k: float = 1.5
    z_thresh: float = 3.0

    # MODERATE BANDS (angepasst)
    faith_high_thr: float = 0.70     # vorher 0.80
    faith_mid_thr: float = 0.40      # vorher 0.50
    # below mid_thr is considered poor


class EvaluationVisualizer:
    """
    Publication-oriented evaluation visualizer.
    """

    def __init__(self, cfg: VizConfig | None = None):
        self.cfg = cfg or VizConfig()
        self.logs_dir = Path(self.cfg.logs_dir)
        self.out_dir = Path(self.cfg.out_dir)
        self.out_dir.mkdir(parents=True, exist_ok=True)
        np.random.seed(self.cfg.random_seed)
        apply_scientific_style()
        self._fig_no = 1

    # ------------------------------------------------------------------
    def _load_eval_rows(self) -> pd.DataFrame:
        rows: List[Dict[str, Any]] = []
        for fp in sorted(self.logs_dir.glob(self.cfg.pattern)):
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))
                rows.append({
                    "query_id": data.get("query_id", fp.stem),
                    "ndcg@k": float(data.get("ndcg@k", np.nan)),
                    "faithfulness": float(data.get("faithfulness", np.nan)),
                    "citation_hit_rate": float(data.get("citation_hit_rate", np.nan)),
                    "dominant_decade": data.get("dominant_decade", None),
                    "model": data.get("model")
                             or data.get("llm_profile")
                             or data.get("model_name"),
                })
            except Exception:
                continue

        df = pd.DataFrame(rows)
        if df.empty:
            return pd.DataFrame(
                columns=["query_id", "ndcg@k", "faithfulness", "citation_hit_rate", "dominant_decade", "model"]
            )
        return df.dropna(how="all", subset=["ndcg@k", "faithfulness"])

    # ------------------------------------------------------------------
    def _bootstrap_ci(self, arr: np.ndarray, iters: int) -> Tuple[float, float, float]:
        arr = arr[~np.isnan(arr)]
        if len(arr) == 0:
            return float("nan"), float("nan"), float("nan")
        if len(arr) == 1:
            m = float(arr[0])
            return m, m, m
        n = len(arr)
        idx = np.random.randint(0, n, size=(iters, n))
        boot = np.mean(arr[idx], axis=1)
        return float(np.mean(boot)), float(np.percentile(boot, 2.5)), float(np.percentile(boot, 97.5))

    # ------------------------------------------------------------------
    def _outliers_iqr(self, arr: np.ndarray) -> np.ndarray:
        arr = np.asarray(arr, float)
        clean = arr[~np.isnan(arr)]
        if len(clean) == 0:
            return np.zeros_like(arr, bool)
        q1, q3 = np.percentile(clean, [25, 75])
        iqr = q3 - q1
        if iqr == 0:
            return np.zeros_like(arr, bool)
        lo = q1 - self.cfg.iqr_k * iqr
        hi = q3 + self.cfg.iqr_k * iqr
        return (arr < lo) | (arr > hi)

    def _outliers_z(self, arr: np.ndarray) -> np.ndarray:
        arr = np.asarray(arr, float)
        mu, sd = np.nanmean(arr), np.nanstd(arr)
        if sd == 0 or np.isnan(sd):
            return np.zeros_like(arr, bool)
        z = (arr - mu) / sd
        return np.abs(z) > self.cfg.z_thresh

    # ------------------------------------------------------------------
    def _save_df(self, name: str, df: pd.DataFrame) -> None:
        df.to_csv(self.out_dir / f"{name}.csv", index=False, encoding="utf-8")

    def _save_fig(self, fig: plt.Figure, stem: str) -> None:
        fig.savefig(self.out_dir / f"{stem}.png", dpi=150, bbox_inches="tight")
        fig.savefig(self.out_dir / f"{stem}.svg", bbox_inches="tight")

    def _titled(self, base: str) -> str:
        title = f"Figure {self._fig_no}: {base}"
        self._fig_no += 1
        return title

    # ------------------------------------------------------------------
    def _faithfulness_band(self, val: float) -> str:
        if np.isnan(val):
            return "missing"
        if val >= self.cfg.faith_high_thr:
            return "high"
        if val >= self.cfg.faith_mid_thr:
            return "medium"
        return "low"

    # ------------------------------------------------------------------
    def plot_ndcg_histogram(self, df: pd.DataFrame) -> None:
        vals = df["ndcg@k"].astype(float).dropna().values
        if len(vals) == 0:
            return

        min_v, max_v = np.min(vals), np.max(vals)
        span = max_v - min_v

        if span < 0.05 and max_v > 0.8:
            xmin = max(0.8, min_v - 0.01)
            xmax = 1.0
            bins = np.linspace(xmin, xmax, 12)
        elif span < 0.2:
            xmin = max(0.0, min_v - 0.05)
            xmax = min(1.0, max_v + 0.05)
            bins = np.linspace(xmin, xmax, 15)
        else:
            xmin, xmax = 0.0, 1.0
            bins = 15

        fig = plt.figure(figsize=(6, 4))
        plt.hist(vals, bins=bins, color=PRIMARY_COLOR, edgecolor="black", alpha=0.8)
        plt.xlabel("NDCG@k")
        plt.ylabel("Count")
        plt.title(self._titled("Distribution of NDCG@k"))
        plt.xlim(xmin, xmax)
        ymin, ymax = plt.ylim()
        plt.ylim(0, max(ymax, 3))
        plt.tight_layout()
        self._save_fig(fig, "hist_ndcg")
        plt.close(fig)

    # ------------------------------------------------------------------
    def plot_faithfulness_bands_global(self, df: pd.DataFrame) -> None:
        vals = df["faithfulness"].astype(float).dropna().values
        if len(vals) == 0:
            return

        bands = [self._faithfulness_band(v) for v in vals]
        series = pd.Series(bands)
        counts = series.value_counts().reindex(["high", "medium", "low"], fill_value=0)

        colors = [
            FAITH_GOOD_COLOR,
            FAITH_MED_COLOR,
            FAITH_LOW_COLOR,
        ]
        labels = [
            "High (≥ 0.70)",
            "Medium (0.40–0.69)",
            "Low (< 0.40)"
        ]

        fig = plt.figure(figsize=(6, 4))
        x = np.arange(len(counts))
        plt.bar(x, counts.values, color=colors, edgecolor="black", alpha=0.85)
        plt.xticks(x, labels, rotation=10)
        plt.ylabel("Number of queries")
        plt.xlabel("Faithfulness band")
        plt.title(self._titled("Faithfulness bands across all queries"))
        for i, v in enumerate(counts.values):
            plt.text(i, v + 0.1, str(int(v)), ha="center", va="bottom", fontsize=9)
        plt.tight_layout()
        self._save_fig(fig, "faithfulness_bands_global")
        plt.close(fig)

    # ------------------------------------------------------------------
    def plot_faithfulness_bands_by_model(self, df: pd.DataFrame) -> None:
        if "model" not in df.columns:
            return
        df_model = df.dropna(subset=["model"])
        if df_model.empty:
            return

        models = sorted(df_model["model"].unique())
        if len(models) <= 1:
            return

        df_model = df_model.copy()
        df_model["faith_band"] = df_model["faithfulness"].astype(float).apply(self._faithfulness_band)

        band_order = ["high", "medium", "low"]
        band_labels = [
            "High (≥ 0.70)",
            "Medium (0.40–0.69)",
            "Low (< 0.40)"
        ]
        band_color_map = {
            "high": FAITH_GOOD_COLOR,
            "medium": FAITH_MED_COLOR,
            "low": FAITH_LOW_COLOR,
        }

        counts = (
            df_model.groupby(["model", "faith_band"])["query_id"]
            .count()
            .unstack(fill_value=0)
            .reindex(columns=band_order, fill_value=0)
        )

        x = np.arange(len(models))
        width = 0.22

        fig = plt.figure(figsize=(7, 4.5))
        for i, band in enumerate(band_order):
            offsets = x + (i - 1) * width
            plt.bar(
                offsets,
                counts[band].values,
                width=width,
                label=band_labels[i],
                color=band_color_map[band],
                edgecolor="black",
                alpha=0.9,
            )

        plt.xticks(x, models, rotation=10)
        plt.ylabel("Number of queries")
        plt.xlabel("LLM profile")
        plt.title(self._titled("Faithfulness band distribution by LLM"))
        plt.legend(frameon=False)
        plt.tight_layout()
        self._save_fig(fig, "faithfulness_bands_by_model")
        plt.close(fig)

    # ------------------------------------------------------------------
    def plot_scatter_correlation(self, df: pd.DataFrame) -> Tuple[float, float]:
        x = df["ndcg@k"].astype(float).values
        y = df["faithfulness"].astype(float).values
        mask = (~np.isnan(x)) & (~np.isnan(y))

        if mask.sum() < 3:
            rp = rs = float("nan")
        else:
            rp = float(np.corrcoef(x[mask], y[mask])[0, 1])
            rs, _ = spearmanr(x[mask], y[mask])

        fig = plt.figure(figsize=(6, 4))
        plt.scatter(x[mask], y[mask], s=26, alpha=0.7, color=ACCENT_COLOR)
        plt.xlabel("NDCG@k")
        plt.ylabel("Faithfulness")
        plt.title(self._titled(f"Scatter NDCG@k vs Faithfulness (r={rp:.3f}, ρ={rs:.3f})"))
        plt.tight_layout()
        self._save_fig(fig, "scatter_ndcg_vs_faithfulness")
        plt.close(fig)
        return rp, rs

    # ------------------------------------------------------------------
    def plot_run_order_control(self, df: pd.DataFrame) -> None:
        df = df.reset_index(drop=True)
        df["idx"] = np.arange(len(df))

        for col, color in [("ndcg@k", PRIMARY_COLOR), ("faithfulness", SECONDARY_COLOR)]:
            vals = df[col].astype(float).values
            if len(vals) == 0:
                continue

            mu, sd = np.nanmean(vals), np.nanstd(vals)
            ucl, lcl = mu + 3 * sd, mu - 3 * sd

            fig = plt.figure(figsize=(8, 4))
            ax = plt.gca()
            ax.fill_between(df["idx"], mu - sd, mu + sd, color="gray", alpha=0.18)
            ax.plot(df["idx"], vals, marker="o", linestyle="-", color=color, linewidth=1.1)
            ax.axhline(mu, linestyle="--", color=color, linewidth=0.9)
            ax.axhline(ucl, linestyle=":", color="black", linewidth=0.8)
            ax.axhline(lcl, linestyle=":", color="black", linewidth=0.8)
            ax.set_xlabel("Run index")
            ax.set_ylabel(col)
            ax.set_title(self._titled(f"Run-order chart for {col}"))
            plt.tight_layout()
            self._save_fig(fig, f"run_order_{col}")
            plt.close(fig)

    # ------------------------------------------------------------------
    def detect_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        nd = df["ndcg@k"].astype(float).values
        fa = df["faithfulness"].astype(float).values

        mask = (
            self._outliers_iqr(nd)
            | self._outliers_z(nd)
            | self._outliers_iqr(fa)
            | self._outliers_z(fa)
        )
        return df[mask].copy()

    # ------------------------------------------------------------------
    def summarize(self, df: pd.DataFrame) -> Dict[str, Any]:
        nd = df["ndcg@k"].astype(float).values
        fa = df["faithfulness"].astype(float).values

        nd_m, nd_lo, nd_hi = self._bootstrap_ci(nd, self.cfg.bootstrap_iters)
        fa_m, fa_lo, fa_hi = self._bootstrap_ci(fa, self.cfg.bootstrap_iters)

        summary = {
            "files": int(df.shape[0]),
            "ndcg@k_mean": nd_m,
            "ndcg@k_ci95_lo": nd_lo,
            "ndcg@k_ci95_hi": nd_hi,
            "faith_mean": fa_m,
            "faith_ci95_lo": fa_lo,
            "faith_ci95_hi": fa_hi,
            "ndcg@k_median": float(np.nanmedian(nd)),
            "faith_median": float(np.nanmedian(fa)),
            "ndcg@k_std": float(np.nanstd(nd)),
            "faith_std": float(np.nanstd(fa)),
            "bootstrap_iters": self.cfg.bootstrap_iters,
            "iqr_k": self.cfg.iqr_k,
            "z_thresh": self.cfg.z_thresh,
            "faith_high_thr": self.cfg.faith_high_thr,
            "faith_mid_thr": self.cfg.faith_mid_thr,
        }

        (self.out_dir / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
        return summary

    # ------------------------------------------------------------------
    def run_all(self) -> Dict[str, Any]:
        df = self._load_eval_rows()
        self._save_df("raw_eval", df)

        if df.empty:
            summary = {"files": 0}
            (self.out_dir / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
            return summary

        self.plot_ndcg_histogram(df)
        self.plot_faithfulness_bands_global(df)
        self.plot_faithfulness_bands_by_model(df)
        self.plot_scatter_correlation(df)
        self.plot_run_order_control(df)

        outliers = self.detect_outliers(df)
        self._save_df("outliers", outliers)

        summary = self.summarize(df)
        return summary

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\ground_truth_builder.py ==== 
# src/core/evaluation/ground_truth_builder.py
from __future__ import annotations
from sentence_transformers import SentenceTransformer, util
from typing import Dict, Any, List
import numpy as np
import logging
from src.core.config.config_loader import ConfigLoader

logger = logging.getLogger("GroundTruthBuilder")


class GroundTruthBuilder:
    """
    Generates semantic ground truth labels for intrinsic retrieval evaluation (NDCG).
    The GT is computed by embedding the user query and measuring its similarity
    to each retrieved chunk. Higher similarity implies stronger relevance.
    
    This implementation is fully offline, consistent with your new
    local-embedding faithfulness metric, and uses the same semantic space
    as your retrieval stack.
    """

    def __init__(
        self,
        config_path: str = "configs/embedding.yaml",
        high_thr: float = 0.40,
        mid_thr: float = 0.25,
        low_thr: float = 0.10
    ):
        # Load the same embedding model used in retrieval
        cfg = ConfigLoader(config_path).config
        model_name = cfg.get("options", {}).get(
            "embedding_model", "multi-qa-mpnet-base-dot-v1"
        )
        self.model = SentenceTransformer(model_name)

        # Consistent thresholds with the new offline faithfulness metric
        self.high_thr = high_thr
        self.mid_thr = mid_thr
        self.low_thr = low_thr

    # ------------------------------------------------------------------
    def build(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, int]:
        if not query or not retrieved_docs:
            return {}

        # Encode query embedding
        q_emb = self.model.encode([query], normalize_embeddings=True)

        truth: Dict[str, int] = {}

        for d in retrieved_docs:
            text = d.get("text", "") or ""
            doc_id = d.get("id") or f"{d.get('metadata', {}).get('source_file')}"

            # Encode chunk embedding
            d_emb = self.model.encode([text], normalize_embeddings=True)
            sim = float(util.cos_sim(q_emb, d_emb)[0][0])

            # Graded relevance assignment
            if sim >= self.high_thr:
                rel = 3
            elif sim >= self.mid_thr:
                rel = 2
            elif sim >= self.low_thr:
                rel = 1
            else:
                rel = 0

            truth[doc_id] = rel

        logger.info(f"Semantic GT created (avg rel={np.mean(list(truth.values())):.2f})")
        return truth

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\multi_model_report_builder.py ==== 
# src/core/evaluation/multi_model_report_builder.py
from __future__ import annotations
import json
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

import matplotlib.pyplot as plt
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import cm
from reportlab.platypus import (
    SimpleDict, Paragraph, Spacer, Image,
    Table, TableStyle, PageBreak
)
from reportlab.lib.styles import getSampleStyleSheet
    from reportlab.lib import colors


FAITH_COLORS = {
    "high": "#1a9850",
    "medium": "#fee08b",
    "low": "#d73027",
}

# UPDATED LABELS
FAITH_LABELS = {
    "high": "High (≥0.70)",
    "medium": "Medium (0.40–0.69)",
    "low": "Low (<0.40)",
}


class MultiModelReportBuilder:
    """Aggregated PDF report comparing multiple LLMs on NDCG and Faithfulness."""

    def __init__(self, base_dir: str = "data"):
        self.base = Path(base_dir)
        self.eval_dirs = sorted(self.base.glob("eval_logs_*"))
        self.out_dir = self.base / "model_comparison"
        self.out_dir.mkdir(parents=True, exist_ok=True)

        styles = getSampleStyleSheet()
        self.styleN = styles["Normal"]
        self.styleH = styles["Heading1"]
        self.styleH2 = styles["Heading2"]

    # ---------------------------------------------------------
    def _load_all_results(self) -> pd.DataFrame:
        rows = []
        for d in self.eval_dirs:
            model = d.name.replace("eval_logs_", "")
            for fp in d.glob("*_evaluation.json"):
                try:
                    x = json.loads(fp.read_text(encoding="utf-8"))
                    rows.append({
                        "query_id": x.get("query_id"),
                        "ndcg": float(x.get("ndcg@k", np.nan)),
                        "faith": float(x.get("faithfulness", np.nan)),
                        "model": model,
                    })
                except Exception:
                    continue
        return pd.DataFrame(rows)

    # ---------------------------------------------------------
    def _faith_band(self, v: float) -> str:
        """UPDATED threshold logic."""
        if np.isnan(v):
            return "missing"
        if v >= 0.70:
            return "high"
        if v >= 0.40:
            return "medium"
        return "low"

    # ---------------------------------------------------------
    def _plot_faithfulness_band_comparison(self, df: pd.DataFrame) -> Path:
        df["band"] = df["faith"].apply(self._faith_band)
        bands = ["high", "medium", "low"]
        models = sorted(df["model"].unique())

        counts = (
            df.groupby(["model", "band"])["query_id"]
            .count()
            .unstack(fill_value=0)
            .reindex(columns=bands, fill_value=0)
        )

        x = np.arange(len(models))
        width = 0.22

        fig, ax = plt.subplots(figsize=(7, 4.5))

        for i, b in enumerate(bands):
            offsets = x + (i - 1) * width
            ax.bar(
                offsets,
                counts[b].values,
                width=width,
                color=FAITH_COLORS[b],
                edgecolor="black",
                alpha=0.9,
                label=FAITH_LABELS[b]
            )

        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=10)
        ax.set_ylabel("Number of queries")
        ax.set_title("Faithfulness band comparison across models")
        ax.legend(frameon=False)

        out = self.out_dir / "faithfulness_model_comparison.png"
        fig.tight_layout()
        fig.savefig(out, dpi=150, bbox_inches="tight")
        plt.close(fig)
        return out

    # ---------------------------------------------------------
    def _aggregate_stats(self, df: pd.DataFrame) -> list[list[str]]:
        rows = [["Model", "Mean NDCG", "Mean Faith", "Median NDCG", "Median Faith", "Std NDCG", "Std Faith"]]
        for m in sorted(df["model"].unique()):
            d = df[df["model"] == m]
            rows.append([
                m,
                f"{d['ndcg'].mean():.3f}",
                f"{d['faith'].mean():.3f}",
                f"{d['ndcg'].median():.3f}",
                f"{d['faith'].median():.3f}",
                f"{d['ndcg'].std():.3f}",
                f"{d['faith'].std():.3f}",
            ])
        return rows

    # ---------------------------------------------------------
    def build(self, name: str = "multi_model_benchmark_report.pdf") -> Path:
        pdf_path = self.out_dir / name
        df = self._load_all_results()

        if df.empty:
            raise ValueError("No evaluation files found for any model.")

        # Plot
        plot_path = self._plot_faithfulness_band_comparison(df)

        # Stats
        stats = self._aggregate_stats(df)

        # PDF builder
        doc = SimpleDocTemplate(str(pdf_path), pagesize=A4,
                                leftMargin=2 * cm, rightMargin=2 * cm,
                                topMargin=2 * cm, bottomMargin=2 * cm)
        story = []

        # Title
        story.append(Paragraph("<b>Multi-Model Benchmark Report</b>", self.styleH))
        story.append(Spacer(1, 0.3 * cm))
        story.append(Paragraph(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}", self.styleN))
        story.append(Spacer(1, 0.4 * cm))

        story.append(Paragraph(
            "This report compares multiple LLM profiles in terms of retrieval relevance "
            "(NDCG@k) and factual grounding (Faithfulness). All evaluations were performed "
            "using an identical prompt set, identical retrieval stack, and identical parameters.",
            self.styleN
        ))
        story.append(PageBreak())

        # Stats Table
        story.append(Paragraph("1. Summary Statistics per Model", self.styleH))
        table = Table(stats, colWidths=[3.2 * cm] * 7)
        table.setStyle(TableStyle([
            ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
            ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
            ("ALIGN", (0, 0), (-1, -1), "CENTER"),
        ]))
        story.append(table)
        story.append(PageBreak())

        # Plot Page
        story.append(Paragraph("2. Faithfulness Band Comparison", self.styleH))
        story.append(Spacer(1, 0.2 * cm))
        story.append(Image(str(plot_path), width=15 * cm, height=9 * cm))
        story.append(PageBreak())

        # End
        story.append(Paragraph("End of Report", self.styleN))

        doc.build(story)
        return pdf_path

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\plot_style.py ==== 
# src/core/evaluation/plot_style.py
"""
Unified visualization style for scientific evaluation plots.
Applies consistent layout, fonts, and color palette across all figures.
Follows principles from Tufte (1983), IEEE Vis Guidelines, and RSS Data Viz Guide (2023).
"""

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np


# ----------------------------------------------------------------------
def apply_scientific_style() -> None:
    """Configure Matplotlib for publication-grade scientific figures."""
    mpl.rcParams.update({
        # Rendering quality
        "figure.dpi": 300,
        "savefig.dpi": 300,
        "savefig.format": "svg",
        "savefig.bbox": "tight",

        # Fonts and layout
        "font.family": "sans-serif",
        "font.sans-serif": ["Arial", "DejaVu Sans", "Liberation Sans"],
        "font.size": 10,
        "axes.labelsize": 10,
        "axes.titlesize": 11,
        "legend.fontsize": 9,
        "xtick.labelsize": 9,
        "ytick.labelsize": 9,

        # Axes and grid
        "axes.linewidth": 0.8,
        "axes.grid": True,
        "grid.alpha": 0.25,
        "grid.linestyle": "--",
        "axes.spines.top": False,
        "axes.spines.right": False,

        # Perceptually uniform, colorblind-safe palette
        "axes.prop_cycle": mpl.cycler(color=[
            "#1b9e77",  # teal
            "#d95f02",  # orange
            "#7570b3",  # purple
            "#e7298a",  # magenta
            "#66a61e",  # green
            "#e6ab02"   # yellow-brown
        ]),

        # Figure layout
        "figure.figsize": (6, 4),
        "figure.autolayout": True,
        "savefig.transparent": False,
    })


# ----------------------------------------------------------------------
def annotate_sample_info(
    ax: plt.Axes,
    n: int | None = None,
    k: int | None = None,
    bootstrap_iters: int | None = None,
    show_conf_int: tuple[float, float] | None = None
) -> None:
    """
    Add standardized annotation text (sample info, parameters) inside a figure.
    Example: n=100, k=5, boot=2000, CI=95%
    """
    txt_parts = []
    if n is not None:
        txt_parts.append(f"n={n}")
    if k is not None:
        txt_parts.append(f"k={k}")
    if bootstrap_iters is not None:
        txt_parts.append(f"boot={bootstrap_iters}")
    if show_conf_int is not None:
        lo, hi = show_conf_int
        txt_parts.append(f"95% CI=[{lo:.3f}, {hi:.3f}]")

    if not txt_parts:
        return

    # Minimalist annotation, bottom-right, partially transparent
    ax.text(
        0.98, 0.02,
        ", ".join(txt_parts),
        ha="right", va="bottom",
        fontsize=8,
        color="gray",
        alpha=0.85,
        transform=ax.transAxes,
        bbox=dict(facecolor="white", edgecolor="none", alpha=0.55, boxstyle="round,pad=0.2")
    )


# ----------------------------------------------------------------------
def add_violin_overlay(ax: plt.Axes, data: np.ndarray, color: str = "#1b9e77") -> None:
    """
    Add a violin-style density overlay (no seaborn dependency).
    Uses kernel density estimation for smooth distribution visualization.
    """
    from scipy.stats import gaussian_kde

    if len(data) < 5:
        return  # too few samples for a meaningful KDE
    data = np.asarray(data, dtype=float)
    data = data[~np.isnan(data)]
    if data.size == 0:
        return

    # Numerical stability: broaden domain slightly if constant values
    dmin, dmax = float(np.min(data)), float(np.max(data))
    if dmax - dmin < 1e-6:
        dmin -= 1e-3
        dmax += 1e-3

    kde = gaussian_kde(data)
    xs = np.linspace(dmin, dmax, 200)
    ys = kde(xs)
    ys = ys / ys.max() * 0.25  # normalized width (relative to axis scale)

    # Symmetric fill for violin overlay
    ax.fill_betweenx(xs, -ys, ys, facecolor=color, alpha=0.18, linewidth=0)
    ax.plot(ys, xs, color=color, alpha=0.5, linewidth=0.6)
    ax.plot(-ys, xs, color=color, alpha=0.5, linewidth=0.6)

    # Maintain centered x-limits (prevents horizontal shift)
    cur_xlim = ax.get_xlim()
    pad = (cur_xlim[1] - cur_xlim[0]) * 0.05
    ax.set_xlim(cur_xlim[0] - pad, cur_xlim[1] + pad)

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\report_builder.py ==== 
# src/core/evaluation/report_builder.py
from __future__ import annotations
import logging
from pathlib import Path
from datetime import datetime

from reportlab.lib.pagesizes import A4
from reportlab.lib.units import cm
from reportlab.platypus import (
    SimpleDocTemplate, Paragraph, Spacer, Image,
    Table, TableStyle, PageBreak
)
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors

import json
import pandas as pd
import numpy as np
from scipy.stats import pearsonr, spearmanr

logger = logging.getLogger("ReportBuilder")


class ReportBuilder:
    """Minimal, robust statistical PDF report: summary stats, correlations, outliers, and charts."""

    def __init__(self, charts_dir: str = "data/eval_charts", eval_dir: str | None = None):
        # Base folder for figures & summary.json
        self.charts_dir = Path(charts_dir)

        # Evaluation-log directory (robust for eval_logs_*)
        if eval_dir:
            self.eval_dir = Path(eval_dir)
        else:
            # Autodiscover eval_logs directories
            parent = self.charts_dir.parent
            candidates = sorted(parent.glob("eval_logs_*"))
            if candidates:
                self.eval_dir = candidates[-1]
            else:
                self.eval_dir = parent / "eval_logs"

        # ReportLab styles
        styles = getSampleStyleSheet()
        self.styleN = styles["Normal"]
        self.styleH = styles["Heading1"]
        self.styleH2 = styles["Heading2"]

    # ------------------------------------------------------------------
    def _load_summary(self) -> dict:
        # Read summary.json produced by EvaluationVisualizer
        summary_path = self.charts_dir / "summary.json"
        if not summary_path.exists():
            raise FileNotFoundError(f"summary.json missing in {self.charts_dir}")
        return json.loads(summary_path.read_text(encoding="utf-8"))

    # ------------------------------------------------------------------
    def _load_detailed_results(self) -> pd.DataFrame:
        # Load all *_evaluation.json for correlation/outlier analysis
        records = []
        for fp in sorted(self.eval_dir.glob("*_evaluation.json")):
            try:
                d = json.loads(fp.read_text(encoding="utf-8"))
                records.append({
                    "query_id": d.get("query_id", fp.stem),
                    "ndcg@k": float(d.get("ndcg@k", np.nan)),
                    "faithfulness": float(d.get("faithfulness", np.nan))
                })
            except Exception:
                continue
        df = pd.DataFrame(records)
        return df.dropna(subset=["ndcg@k", "faithfulness"])

    # ------------------------------------------------------------------
    def _compute_correlations(self, df: pd.DataFrame) -> dict:
        # Pearson + Spearman between NDCG and Faithfulness
        if df.empty:
            return {}
        x = df["ndcg@k"].astype(float)
        y = df["faithfulness"].astype(float)
        pr, pp = pearsonr(x, y)
        sr, sp = spearmanr(x, y)
        return {
            "pearson_r": pr,
            "pearson_p": pp,
            "spearman_rho": sr,
            "spearman_p": sp
        }

    # ------------------------------------------------------------------
    def _detect_outliers(self, df: pd.DataFrame, z_thresh: float = 3.0) -> pd.DataFrame:
        # Robust z-score outlier detection
        if df.empty:
            return df
        eps = 1e-6
        df = df.copy()
        df["z_ndcg"] = (df["ndcg@k"] - df["ndcg@k"].mean()) / (df["ndcg@k"].std() + eps)
        df["z_faith"] = (df["faithfulness"] - df["faithfulness"].mean()) / (df["faithfulness"].std() + eps)
        df["is_outlier"] = (df["z_ndcg"].abs() > z_thresh) | (df["z_faith"].abs() > z_thresh)
        return df[df["is_outlier"]]

    # ------------------------------------------------------------------
    def _find_charts(self) -> list[Path]:
        # Include all PNG charts (histograms, correlation plots, etc.)
        return sorted(self.charts_dir.glob("*.png"))

    # ------------------------------------------------------------------
    def build(self, custom_name: str | None = None) -> Path:
        # Load summary data
        summary = self._load_summary()
        n = summary.get("files", 0)

        # PDF name
        pdf_name = custom_name or f"benchmark_report_n{n}.pdf"
        pdf_path = self.charts_dir / pdf_name

        # PDF document setup
        doc = SimpleDocTemplate(
            str(pdf_path),
            pagesize=A4,
            rightMargin=2 * cm,
            leftMargin=2 * cm,
            topMargin=2 * cm,
            bottomMargin=2 * cm
        )
        story = []

        # ------------------------------------------------------------------
        # Title Page
        story.append(Paragraph("<b>Statistical Benchmark Report</b>", self.styleH))
        story.append(Spacer(1, 0.4 * cm))
        story.append(Paragraph(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}", self.styleN))
        story.append(Spacer(1, 0.4 * cm))
        story.append(Paragraph(
            "This report provides quantitative evaluation results for NDCG@k and Faithfulness, "
            "correlation diagnostics, outlier detection, and statistical visualizations.",
            self.styleN,
        ))
        story.append(PageBreak())

        # ------------------------------------------------------------------
        # Summary Statistics
        story.append(Paragraph("1. Summary Statistics", self.styleH))
        story.append(Spacer(1, 0.2 * cm))

        rows = []
        for k, v in summary.items():
            label = k.replace("_", " ")
            if isinstance(v, (int, float)):
                rows.append([label, f"{v:.4f}"])
            else:
                rows.append([label, str(v)])

        table = Table(rows, colWidths=[8 * cm, 6 * cm])
        table.setStyle(TableStyle([
            ("BACKGROUND", (0, 0), (1, 0), colors.lightgrey),
            ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
            ("ALIGN", (0, 0), (-1, -1), "CENTER")
        ]))
        story.append(table)
        story.append(PageBreak())

        # ------------------------------------------------------------------
        # Correlations + Outliers
        df = self._load_detailed_results()
        if not df.empty:
            story.append(Paragraph("2. Correlation and Outlier Analysis", self.styleH))
            story.append(Spacer(1, 0.2 * cm))

            corr = self._compute_correlations(df)
            if corr:
                crows = [
                    ["Metric Pair", "Correlation", "p-value"],
                    ["Pearson r", f"{corr['pearson_r']:.3f}", f"{corr['pearson_p']:.3e}"],
                    ["Spearman ρ", f"{corr['spearman_rho']:.3f}", f"{corr['spearman_p']:.3e}"],
                ]
                ct = Table(crows, colWidths=[6 * cm, 4 * cm, 4 * cm])
                ct.setStyle(TableStyle([
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("ALIGN", (0, 0), (-1, -1), "CENTER")
                ]))
                story.append(ct)
                story.append(Spacer(1, 0.3 * cm))

            out = self._detect_outliers(df)
            if not out.empty:
                story.append(Paragraph("Detected Outliers", self.styleH2))
                story.append(Spacer(1, 0.2 * cm))

                trows = [["Query", "NDCG@k", "Faithfulness", "z_ndcg", "z_faith"]]
                for _, r in out.iterrows():
                    qid = r["query_id"]
                    wrapped = "\n".join([qid[i:i+45] for i in range(0, len(qid), 45)])
                    trows.append([
                        wrapped,
                        f"{r['ndcg@k']:.3f}",
                        f"{r['faithfulness']:.3f}",
                        f"{r['z_ndcg']:.2f}",
                        f"{r['z_faith']:.2f}",
                    ])

                ot = Table(
                    trows,
                    colWidths=[8 * cm, 2.4 * cm, 2.4 * cm, 2.0 * cm, 2.0 * cm]
                )
                ot.setStyle(TableStyle([
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("ALIGN", (1, 1), (-1, -1), "CENTER"),
                    ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                    ("FONTSIZE", (0, 0), (-1, -1), 8)
                ]))
                story.append(ot)
            else:
                story.append(Paragraph("No outliers detected.", self.styleN))

            story.append(PageBreak())

        # ------------------------------------------------------------------
        # Charts
        images = self._find_charts()
        if images:
            story.append(Paragraph("3. Visual Analytics", self.styleH))
            story.append(Spacer(1, 0.2 * cm))
            for img in images:
                story.append(Paragraph(img.name.replace("_", " "), self.styleH2))
                story.append(Image(str(img), width=15 * cm, height=9 * cm))
                story.append(Spacer(1, 0.4 * cm))

        # ------------------------------------------------------------------
        doc.build(story)
        logger.info(f"PDF generated → {pdf_path}")
        return pdf_path

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\utils.py ==== 
from __future__ import annotations
from typing import Dict, Any
import hashlib

def make_chunk_id(chunk: Dict[str, Any]) -> str:
    meta = chunk.get("metadata", {}) or {}
    src = meta.get("source_file") or meta.get("title") or "unknown"
    year = meta.get("year", "na")
    text = (chunk.get("text") or "")[:200]
    h = hashlib.sha1(text.encode("utf-8", errors="ignore")).hexdigest()[:12]
    return f"{src}::{year}::{h}"

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\interfaces\i_metric.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict

class IMetric(ABC):
    """Interface defining a unified metric contract for all evaluators."""

    @abstractmethod
    def compute(self, **kwargs: Any) -> float:
        """Compute the metric based on explicit input parameters."""
        pass

    @abstractmethod
    def describe(self) -> Dict[str, str]:
        """Return metadata about the metric (name, type, short description)."""
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\faithfulness_metric.py ==== 
# src/core/evaluation/metrics/faithfulness_metric.py
from __future__ import annotations
from typing import List, Dict, Any, Tuple
import numpy as np
import re
import spacy

from sentence_transformers import SentenceTransformer, CrossEncoder
from sentence_transformers.util import cos_sim

# Load spaCy model once (NER + sentence segmentation)
# # uses small model for memory efficiency
try:
    NLP = spacy.load("en_core_web_sm")
except Exception:
    NLP = None


class FaithfulnessMetric:
    """
    Claim-level, evidence-based faithfulness metric combining:
    - Cross-encoder NLI entailment for robust evidence validation
    - Embedding similarity fallback for borderline segments
    - Top-k evidence aggregation
    - Specificity penalty (excessive ungrounded detail)
    - Temporal consistency penalty (year/decade mismatch)
    """

    def __init__(
        self,
        ent_model: str = "cross-encoder/nli-deberta-base",
        emb_model: str = "multi-qa-mpnet-base-dot-v1",
        high_thr: float = 0.55,
        mid_thr: float = 0.35,
        top_k: int = 3,
        specificity_penalty: float = 0.15,
        temporal_penalty: float = 0.15
    ):
        # # main entailment model (entails/neutral/contradiction)
        self.cross = CrossEncoder(ent_model)

        # # embedding model fallback
        self.emb = SentenceTransformer(emb_model)

        # # thresholds for embedding fallback
        self.high_thr = high_thr
        self.mid_thr = mid_thr
        self.top_k = top_k

        # # penalty weights
        self.w_spec = specificity_penalty
        self.w_temp = temporal_penalty

    # ----------------------------------------------------------
    def _extract_claims(self, answer: str) -> List[str]:
        """Split answer into minimal evidence-checkable claims."""
        if not answer:
            return []
        if NLP:
            doc = NLP(answer)
            sents = [s.text.strip() for s in doc.sents]
        else:
            sents = re.split(r"[.!?]\s+", answer)

        claims = []
        for s in sents:
            s_clean = s.strip()
            if len(s_clean.split()) >= 3:
                claims.append(s_clean)
        return claims

    # ----------------------------------------------------------
    def _claim_date(self, claim: str) -> int | None:
        """Extract explicit years from claim (e.g. 1998, 2020)."""
        yrs = re.findall(r"\b(19\d{2}|20\d{2})\b", claim)
        return int(yrs[0]) if yrs else None

    # ----------------------------------------------------------
    def _chunk_decade(self, chunk_meta: Dict[str, Any]) -> int | None:
        """Extract decade from chunk metadata."""
        try:
            y = int(chunk_meta.get("year"))
            if 1900 <= y <= 2100:
                return (y // 10) * 10
        except Exception:
            pass
        return None

    # ----------------------------------------------------------
    def _specificity_score(self, claim: str) -> float:
        """Quantify factual specificity based on NER + numeric density."""
        if not NLP:
            nums = len(re.findall(r"\d+", claim))
            ents = len(re.findall(r"[A-Z][a-z]+", claim))
            return (nums + ents) / max(5, len(claim.split()))

        doc = NLP(claim)
        nums = sum(1 for t in doc if t.like_num)
        ents = len(doc.ents)
        return (nums + ents) / max(5, len(doc))

    # ----------------------------------------------------------
    def _temporal_penalty(self, claim_year: int | None, chunk_decades: List[int]) -> float:
        """Check if claim year is consistent with retrieved evidence decades."""
        if claim_year is None or not chunk_decades:
            return 0.0
        closest = min(chunk_decades, key=lambda d: abs(d - claim_year))
        diff = abs(closest - claim_year)
        if diff <= 10:
            return 0.0
        if diff <= 20:
            return 0.5 * self.w_temp
        return 1.0 * self.w_temp

    # ----------------------------------------------------------
    def _entailment_score(self, claim: str, chunks: List[str]) -> float:
        """Compute max entailment probability over all chunks."""
        if not chunks:
            return 0.0

        pairs = [(claim, c) for c in chunks]
        preds = self.cross.predict(pairs, apply_softmax=True)
        entail_probs = np.array([p[0] for p in preds])
        topk = np.sort(entail_probs)[-self.top_k:]
        return float(np.mean(topk))

    # ----------------------------------------------------------
    def _embedding_fallback(self, claim: str, chunks: List[str]) -> float:
        """Fallback evidence score using embedding similarity."""
        if not chunks:
            return 0.0

        c_emb = self.emb.encode([claim], normalize_embeddings=True)
        ch_emb = self.emb.encode(chunks, normalize_embeddings=True)

        sims = cos_sim(c_emb, ch_emb)[0].cpu().numpy()
        topk = np.sort(sims)[-self.top_k:]
        s = float(np.mean(topk))

        if s >= self.high_thr:
            return 1.0
        if s >= self.mid_thr:
            return 0.5
        return 0.0

    # ----------------------------------------------------------
    def compute(self, context_chunks: List[str], answer: str) -> float:
        """Main faithfulness routine combining entailment, penalties and fallback."""
        if not context_chunks or not answer:
            return 0.0

        claims = self._extract_claims(answer)
        if not claims:
            return 0.0

        # # prepare decade metadata
        chunk_decades = []
        for c in context_chunks:
            pass
        # # decade list injection is done by EvaluationOrchestrator, not here

        scores = []

        for claim in claims:
            claim_year = self._claim_date(claim)
            specificity = self._specificity_score(claim)

            ent = self._entailment_score(claim, context_chunks)
            if ent < 0.20:
                ent = self._embedding_fallback(claim, context_chunks)

            # # specificity penalty mapping
            spec_pen = min(1.0, specificity) * self.w_spec

            # # temporal penalty (simple version: penalize if claim year is far from evidence decades)
            temp_pen = 0.0  # orchestrator injects decades; optional hook

            raw_score = max(0.0, ent - spec_pen - temp_pen)
            scores.append(raw_score)

        return float(np.mean(scores))

    # ----------------------------------------------------------
    def describe(self) -> Dict[str, str]:
        return {
            "name": "FaithfulnessV2",
            "type": "extrinsic",
            "description": (
                "Claim-level factual evaluation using cross-encoder entailment, "
                "top-k evidence aggregation, specificity and temporal penalties."
            )
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\ndcg_metric.py ==== 
from __future__ import annotations
import math
from typing import List, Dict
from src.core.evaluation.interfaces.i_metric import IMetric

class NDCGMetric(IMetric):
    """Normalized Discounted Cumulative Gain (intrinsic retrieval metric)."""

    def __init__(self, k: int = 5):
        self.k = k

    def compute(self, relevance_scores: List[int]) -> float:
        """Compute NDCG@k from graded relevance list."""
        def dcg(scores: List[int]) -> float:
            return sum(s / math.log2(i + 2) for i, s in enumerate(scores[:self.k]))
        if not relevance_scores:
            return 0.0
        ideal = sorted(relevance_scores, reverse=True)
        idcg = dcg(ideal)
        return (dcg(relevance_scores) / idcg) if idcg > 0 else 0.0

    def describe(self) -> Dict[str, str]:
        return {
            "name": "NDCG@k",
            "type": "intrinsic",
            "description": "Measures retrieval ranking quality using graded relevance with logarithmic discount."
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\__init__.py ==== 
.
