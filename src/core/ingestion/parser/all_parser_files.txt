==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
# src/core/ingestion/parser/parallel_pdf_parser.py
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    result = future.result()
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"âœ“ Parsed {pdf.name}")
                except Exception as e:
                    self.logger.error(f"âœ— Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        parser = self.parser_factory.create_parser()
        return parser.parse(pdf_path)

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser


class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()
        self.parallelism = opts.get("parallelism", "auto")
        self.language = opts.get("language", "auto")
        self.exclude_toc = True  # enforced globally
        self.max_pages = opts.get("max_pages", None)

        # determine optimal CPU usage
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    # ------------------------------------------------------------------
    def create_parser(self) -> IPdfParser:
        """Return configured parser instance (currently only PyMuPDF)."""
        if self.parser_mode in ("fitz", "auto"):
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
        raise ValueError(f"Unsupported parser mode: {self.parser_mode}")

    # ------------------------------------------------------------------
    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
from __future__ import annotations
from typing import Dict, Any, List
from pathlib import Path
import fitz  # PyMuPDF
import re
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser


class PyMuPDFParser(IPdfParser):
    """Robust PDF parser using PyMuPDF with layout-based filtering of non-body text."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None):
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages

    # ------------------------------------------------------------------
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Extract clean body text (excluding title, abstract, headers) and minimal metadata."""
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []

        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            blocks = page.get_text("blocks")
            page_body = self._extract_body_from_blocks(blocks)
            if not page_body:
                continue
            # Skip ToC-like pages entirely
            if self.exclude_toc and self._looks_like_toc_page(page_body, toc_titles):
                continue
            text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text.strip(), "metadata": metadata}

    # ------------------------------------------------------------------
    def _extract_toc_titles(self, doc) -> List[str]:
        """Extract TOC titles to later filter out from main text."""
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    # ------------------------------------------------------------------
    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """Heuristic to detect table of contents pages."""
        if not text or len(text) < 100:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    # ------------------------------------------------------------------
    def _extract_body_from_blocks(self, blocks: list) -> str:
        """Select only text blocks likely belonging to main body based on layout and heuristics."""
        # Determine median vertical position â†’ ignore top 15â€“20%
        y_positions = [b[1] for b in blocks if len(b) >= 5]
        if not y_positions:
            return ""
        page_top_cutoff = sorted(y_positions)[int(len(y_positions) * 0.15)]
        candidate_blocks = []

        for (x0, y0, x1, y1, text, *_ ) in blocks:
            if not text or len(text.strip()) < 30:
                continue
            # Exclude upper-page fragments (titles, authors, abstract)
            if y1 < page_top_cutoff + 100:
                if re.search(r"(?i)(abstract|title|author|doi|keywords|arxiv|university|faculty|institute|version)", text):
                    continue
                if len(text.strip().split()) < 20:
                    continue
            candidate_blocks.append(text.strip())

        return "\n".join(candidate_blocks)

    # ------------------------------------------------------------------
    def _remove_residual_metadata(self, text: str) -> str:
        """Remove residual header/footer and metadata-like fragments."""
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(abstract|zusammenfassung)\b.*?(?=\n[A-Z][a-z]|$)",
            r"(?im)^title\s*:.*$",
            r"(?im)^author\s*:.*$",
            r"(?im)^\s*(version|revision|doi|arxiv).*?$",
            r"(?im)\bpage\s+\d+\b",
            r"(?m)^\s*\d+\s*$",
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)
        text = re.sub(r"\n{2,}", "\n\n", text)
        return text.strip()

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\__pycache__\i_pdf_parser.cpython-312.pyc ==== 
Ë
    ùìiy  ã                  óF   — d dl mZ d dlmZmZ d dlmZmZ  G d„ de«      Zy)é    )Úannotations)ÚABCÚabstractmethod)ÚDictÚAnyc                  ó"   — e Zd ZdZedd„«       Zy)Ú
IPdfParserz$Interface for all local PDF parsers.c                ó   — t         ‚)zEParse a PDF file and return structured output with text and metadata.)ÚNotImplementedError)ÚselfÚpdf_paths     ú]C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.pyÚparsezIPdfParser.parse	   s
   € ô "Ğ!ó    N)r   ÚstrÚreturnzDict[str, Any])Ú__name__Ú
__module__Ú__qualname__Ú__doc__r   r   © r   r   r	   r	      s   „ Ù.àò"ó ñ"r   r	   N)	Ú
__future__r   Úabcr   r   Útypingr   r   r	   r   r   r   Ú<module>r      s   ğİ "ß #ß ô"õ "r   .
