==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json
import time

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        # Konfiguriere die maximale Anzahl an Workern, basierend auf der Anzahl der CPU-Kerne
        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            # Maximal 4 Worker verwenden, wenn die CPU nicht so viele Kerne hat
            self.num_workers = min(max(1, multiprocessing.cpu_count() - 1), 4)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        """
        Parse all PDFs in the given directory and output the parsed results to the output directory.
        
        :param pdf_dir: Directory containing the PDF files.
        :param output_dir: Directory where the parsed results should be saved.
        :return: List of dictionaries containing parsed content from the PDFs.
        """
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    # Set timeout for each future (e.g., 300 seconds = 5 minutes)
                    result = future.result(timeout=300)  # Timeout set to 5 minutes
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"Parsed {pdf.name}")
                except concurrent.futures.TimeoutError:
                    self.logger.error(f"Timeout occurred while parsing {pdf.name}")
                except Exception as e:
                    self.logger.error(f"Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        """
        Parse a single PDF file using the appropriate parser and return the extracted data.
        
        :param pdf_path: Path to the PDF file to parse.
        :return: A dictionary containing parsed content from the PDF.
        """
        parser = self.parser_factory.create_parser()
        try:
            result = parser.parse(pdf_path)
            return result
        except Exception as e:
            self.logger.error(f"Error parsing {pdf_path}: {e}")
            return {"text": "", "metadata": {"source_file": pdf_path}}


==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser
from src.core.ingestion.parser.pdfplumber_parser import PdfPlumberParser  # Import PdfPlumberParser

class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()  # Get parser mode (fitz, pdfplumber, auto)
        self.parallelism = opts.get("parallelism", "auto")  # Get parallelism setting
        self.language = opts.get("language", "auto")  # Get language setting
        self.exclude_toc = True  # Enforce exclusion of table of contents globally
        self.max_pages = opts.get("max_pages", None)  # Max pages to parse

        # Determine optimal CPU usage based on parallelism
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    def create_parser(self) -> IPdfParser:
        """Return configured parser instance based on configuration (fitz, pdfplumber, etc.)."""
        if self.parser_mode == "fitz":
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PyMuPDFParser
        elif self.parser_mode == "pdfplumber":
            return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PdfPlumberParser
        elif self.parser_mode == "auto":
            # Automatically choose the best parser
            try:
                return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
            except Exception:
                return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Fallback to PyMuPDFParser
        else:
            raise ValueError(f"Unsupported parser mode: {self.parser_mode}")  # Error if parser mode is unsupported

    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)  # Return parallel parser for distribution

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pdfplumber_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
import pdfplumber  # Import pdfplumber for better multi-column text extraction
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PdfPlumberParser(IPdfParser):
    """Robust PDF parser using pdfplumber to handle multi-column text extraction and OCR fallback."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(pdf_path) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page using pdfplumber
        with pdfplumber.open(pdf_path) as pdf:
            for page_index, page in enumerate(pdf.pages):
                if self.max_pages and page_index >= self.max_pages:
                    break

                # Extract text from the page considering multi-column layout
                page_body = self._extract_text_from_page(page)
                if page_body:
                    text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(pdf.pages),
        }

        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_text_from_page(self, page) -> str:
        """
        Extract text from a page considering the multi-column layout using pdfplumber.
        
        :param page: The pdfplumber page object.
        :return: Extracted text from the page.
        """
        # Split the page into columns using pdfplumber's layout extraction
        width = page.width
        left_column = page.within_bbox((0, 0, width / 3, page.height))  # Left column
        middle_column = page.within_bbox((width / 3, 0, 2 * width / 3, page.height))  # Middle column
        right_column = page.within_bbox((2 * width / 3, 0, width, page.height))  # Right column

        # Extract text from each column
        left_text = left_column.extract_text()
        middle_text = middle_column.extract_text()
        right_text = right_column.extract_text()

        # Combine the text from all columns
        combined_text = f"{left_text}\n{middle_text}\n{right_text}"

        return combined_text

    def _extract_toc_titles(self, pdf_path: Path) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param pdf_path: The path to the PDF file.
        :return: A list of TOC titles.
        """
        toc_titles = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                # Try to get TOC titles from the page (if any)
                toc = page.extract_text()
                if toc and re.search(r"(?i)\btable\s+of\s+contents\b", toc):
                    toc_titles.append(toc)
        return toc_titles

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PyMuPDFParser(IPdfParser):
    """Robust PDF parser using PyMuPDF with layout-based filtering of non-body text and OCR fallback for scanned PDFs."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page
        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            blocks = page.get_text("blocks")

            # If no text is found, use OCR
            if not blocks and self.use_ocr:
                blocks = self._apply_ocr_to_page(page)

            page_body = self._extract_body_from_blocks(blocks)
            if not page_body:
                continue  # Skip empty pages or pages without meaningful content
            # Skip ToC-like pages entirely
            if self.exclude_toc and self._looks_like_toc_page(page_body, toc_titles):
                continue
            text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_toc_titles(self, doc) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param doc: The PyMuPDF document object.
        :return: A list of TOC titles.
        """
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """
        Heuristic to detect table of contents pages by checking if the text contains typical TOC structure.

        :param text: The text from a page.
        :param toc_titles: The list of TOC titles to match against.
        :return: True if the page looks like a TOC, False otherwise.
        """
        if not text or len(text) < 100:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    def _extract_body_from_blocks(self, blocks: list) -> str:
        """
        Optimized extraction to ensure only meaningful body text (e.g., abstract, sections) is included.
        
        :param blocks: The list of text blocks extracted from the page.
        :return: A string of extracted body text.
        """
        # Initialize variables for abstract detection and block storage
        abstract_found = False
        abstract_block = []
        candidate_blocks = []

        for (x0, y0, x1, y1, text, *_ ) in blocks:
            if not text or len(text.strip()) < 30:
                continue  # Skip empty or short text blocks

            # If the abstract hasn't been found, try to capture it
            if not abstract_found and re.search(r"(?i)\babstract\b", text):
                abstract_found = True
                abstract_block.append(text.strip())
                continue  # Skip content after abstract until the main body starts

            # Filter out metadata and irrelevant blocks
            if re.search(r"(?i)(title|author|doi|keywords|arxiv|university|faculty|institute|version)", text):
                continue  # Skip metadata blocks
            if len(text.strip().split()) < 20:
                continue  # Skip short blocks

            # Add the remaining relevant blocks
            candidate_blocks.append(text.strip())

        # Add the abstract at the beginning of the body if it was found
        if abstract_found:
            candidate_blocks.insert(0, "\n".join(abstract_block))

        return "\n".join(candidate_blocks)

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        # Extended patterns for filtering out metadata
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
            r"(?i)(RSISE|NICTA|university|faculty|institute)\b.*",  # Remove institution names
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        # Optionally, remove excessive line breaks or extra spaces
        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
