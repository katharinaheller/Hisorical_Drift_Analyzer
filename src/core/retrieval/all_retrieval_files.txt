==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\all_retrieval_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\all_retrieval_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\faiss_retriever.py ==== 
# src/core/retrieval/faiss_retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import re
import numpy as np
import logging
from math import exp
from src.core.retrieval.interfaces.i_retriever import IRetriever


class FAISSRetriever(IRetriever):
    """FAISS-based semantic retriever with optional temporal and source diversification."""

    def __init__(
        self,
        vector_store_dir: str,
        model_name: str,
        top_k_retrieve: int = 50,
        normalize_embeddings: bool = True,
        use_gpu: bool = False,
        similarity_metric: str = "cosine",
        temporal_awareness: bool = True,
        temporal_tau: float = 8.0,
        temporal_weight: float = 0.30,
        valid_year_range: Tuple[int, int] = (1900, 2100),
        diversify_sources: bool = True,  # enable balanced source selection
    ):
        self.logger = logging.getLogger(self.__class__.__name__)

        try:
            import faiss  # type: ignore
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError(
                "faiss-cpu and sentence-transformers are required. "
                "Install via: poetry add faiss-cpu sentence-transformers"
            ) from e

        self.faiss = faiss
        self.vector_store_dir = Path(vector_store_dir).resolve()
        self.index_path = self.vector_store_dir / "index.faiss"
        self.meta_path = self.vector_store_dir / "metadata.jsonl"

        if not self.index_path.exists() or not self.meta_path.exists():
            raise FileNotFoundError(f"Incomplete vector store: {self.vector_store_dir}")

        self.model = SentenceTransformer(model_name)
        self.top_k_retrieve = int(top_k_retrieve)
        self.normalize_embeddings = bool(normalize_embeddings)
        self.use_gpu = bool(use_gpu)
        self.similarity_metric = similarity_metric.lower().strip()
        self.temporal_awareness = bool(temporal_awareness)
        self.temporal_tau = float(temporal_tau)
        self.temporal_weight = float(temporal_weight)
        self.valid_year_range = valid_year_range
        self.diversify_sources = bool(diversify_sources)

        if self.similarity_metric not in {"cosine", "dot"}:
            raise ValueError(f"Unsupported similarity metric: {self.similarity_metric}")

        # Load FAISS index
        self.logger.info(f"Loading FAISS index: {self.index_path}")
        self.index = faiss.read_index(str(self.index_path))
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                self.logger.info("FAISS GPU acceleration enabled")
            except Exception as e:
                self.logger.warning(f"GPU mode failed, falling back to CPU: {e}")

        # Load metadata
        with open(self.meta_path, "r", encoding="utf-8") as f:
            self.metadata = [json.loads(line) for line in f]

        self.logger.info(
            f"FAISSRetriever ready | entries={len(self.metadata)} | metric={self.similarity_metric.upper()} "
            f"| temporal_awareness={self.temporal_awareness} | diversify_sources={self.diversify_sources}"
        )

    # ------------------------------------------------------------------
    def _encode_query(self, query: str) -> np.ndarray:
        # Encode query with normalization
        vec = self.model.encode([query], normalize_embeddings=self.normalize_embeddings)
        return np.asarray(vec, dtype="float32")

    def _normalize_scores(self, distances: np.ndarray) -> np.ndarray:
        # Normalize distances depending on metric
        if self.similarity_metric in {"cosine", "dot"}:
            return distances
        return 1 - distances

    # ------------------------------------------------------------------
    def _extract_years_from_query(self, query: str) -> List[int]:
        """
        Extract explicit years (e.g. 2021), decade mentions (e.g. 'in the 2020s'),
        or century references (e.g. '21st century') from a text query.
        Returns a sorted list of representative years.
        """
        if not query:
            return []

        text = query.lower()
        years: set[int] = set()
        lo, hi = self.valid_year_range

        # 1) Explicit years (e.g. 1999, 2023)
        for m in re.findall(r"\b(19\d{2}|20\d{2})\b", text):
            try:
                y = int(m)
                if lo <= y <= hi:
                    years.add(y)
            except ValueError:
                continue

        # 2) Decades (e.g. 1980s, 2020s, early 1990s)
        for m in re.findall(r"\b(19|20)\d0s\b", text):
            try:
                decade = int(m + "0")
                if lo <= decade <= hi:
                    years.update(range(decade, decade + 10))
            except ValueError:
                continue

        # 3) Centuries (e.g. "20th century", "21st century")
        if "20th century" in text:
            years.update(range(1900, 2000))
        if "21st century" in text:
            years.update(range(2000, 2100))

        # Optional compact logging: only start years of detected decades
        unique_decades = sorted({(y // 10) * 10 for y in years})
        return unique_decades

    # ------------------------------------------------------------------
    def _temporal_modulate(self, base_score: float, doc_year: Optional[int], query_years: List[int]) -> float:
        # Exponential weighting for temporal alignment
        if doc_year is None or not query_years:
            return base_score
        nearest = min(abs(doc_year - y) for y in query_years)
        w = exp(-nearest / max(self.temporal_tau, 1e-6))
        return base_score * (1.0 + self.temporal_weight * w)

    def _safe_doc_year(self, entry: Dict[str, Any]) -> Optional[int]:
        # Extract publication year safely from metadata
        meta = entry.get("metadata", {}) or {}
        y = meta.get("year")
        try:
            yi = int(str(y))
            lo, hi = self.valid_year_range
            if lo <= yi <= hi:
                return yi
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _apply_source_diversity(self, results: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """Ensure chunks from different PDFs dominate early ranks."""
        if not self.diversify_sources or not results:
            return results[:top_k]

        diversified, seen = [], set()
        for r in results:
            src = r["metadata"].get("source_file", "unknown")
            if src not in seen:
                diversified.append(r)
                seen.add(src)
            if len(diversified) >= top_k:
                break

        # Fill up with remaining if fewer than top_k unique
        if len(diversified) < top_k:
            for r in results:
                if r not in diversified:
                    diversified.append(r)
                if len(diversified) >= top_k:
                    break
        return diversified

    # ------------------------------------------------------------------
    def search(self, query: str, top_k: Optional[int] = None, temporal_mode: bool = True) -> List[Dict[str, Any]]:
        """Similarity search with optional temporal and source diversification."""
        self.temporal_awareness = bool(temporal_mode)
        k = int(top_k or self.top_k_retrieve)
        k = max(1, min(k, self.index.ntotal))

        q_vec = self._encode_query(query)
        D, I = self.index.search(q_vec, k * 5 if self.diversify_sources else k)
        scores = self._normalize_scores(D[0])
        query_years = self._extract_years_from_query(query) if self.temporal_awareness else []

        results: List[Dict[str, Any]] = []
        for score, idx in zip(scores, I[0]):
            if idx < 0 or idx >= len(self.metadata):
                continue
            entry = self.metadata[idx]
            doc_year = self._safe_doc_year(entry)
            mod_score = (
                self._temporal_modulate(float(score), doc_year, query_years)
                if self.temporal_awareness else float(score)
            )
            results.append({
                "score": float(mod_score),
                "text": (entry.get("text", "") or "")[:500],
                "metadata": entry.get("metadata", {}) or {},
            })

        results.sort(key=lambda r: r["score"], reverse=True)
        diversified = self._apply_source_diversity(results, top_k=k)

        self.logger.info(
            f"Retrieved {len(diversified)} candidates | temporal_mode={self.temporal_awareness} "
            f"| diversify_sources={self.diversify_sources} | years_in_query={query_years or 'none'}"
        )
        return diversified

    # ------------------------------------------------------------------
    def close(self) -> None:
        self.logger.info("FAISS retriever closed")

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\reranker_factory.py ==== 
# src/core/retrieval/reranker_factory.py
from __future__ import annotations
import logging
from typing import Any, Dict, Type

from src.core.retrieval.temporal_reranker import TemporalReranker
from src.core.retrieval.semantic_reranker import SemanticReranker
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)


class RerankerFactory:
    """Factory for deterministic and clean reranker construction."""

    _registry: Dict[str, Type[IReranker]] = {
        "temporal": TemporalReranker,
        "semantic": SemanticReranker,
    }

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IReranker:
        """Instantiate reranker from configuration dictionary with clean parameter mapping."""
        opts = cfg.get("options", {})
        rtype = str(opts.get("reranker", "semantic")).lower()

        if rtype not in RerankerFactory._registry:
            raise ValueError(
                f"Unsupported reranker type: {rtype}. "
                f"Available: {list(RerankerFactory._registry.keys())}"
            )

        cls = RerankerFactory._registry[rtype]
        logger.info(f"Initializing reranker of type='{rtype}'")

        # -------------------------------------------------------------
        # Temporal Reranker (Golden Middle Version)
        # -------------------------------------------------------------
        if cls is TemporalReranker:
            return TemporalReranker(
                semantic_threshold=float(opts.get("semantic_threshold", 0.40)),
                min_year=int(opts.get("min_year", 1900)),
                must_include=list(opts.get("must_include", [])),
                blacklist_sources=list(opts.get("blacklist_sources", [])),
            )

        # -------------------------------------------------------------
        # Semantic Reranker
        # -------------------------------------------------------------
        if cls is SemanticReranker:
            return SemanticReranker(
                model_name=str(
                    opts.get(
                        "semantic_model",
                        "cross-encoder/ms-marco-MiniLM-L-6-v2"
                    )
                ),
                top_k=int(opts.get("top_k_rerank", 25)),
                semantic_weight=float(opts.get("semantic_weight", 0.75)),
                use_gpu=bool(opts.get("use_gpu", False)),
            )

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retrieval_orchestrator.py ==== 
# src/core/retrieval/retrieval_orchestrator.py
from __future__ import annotations
import logging
from typing import List, Dict, Any, Optional
from collections import defaultdict
import numpy as np
from sentence_transformers import SentenceTransformer, util

from src.core.retrieval.faiss_retriever import FAISSRetriever
from src.core.retrieval.reranker_factory import RerankerFactory
from src.core.config.config_loader import ConfigLoader
from src.core.evaluation.utils import make_chunk_id  # # stable id builder for evaluation


class RetrievalOrchestrator:
    """Unified retrieval orchestrator — orchestrates query → retrieval → reranking pipeline."""

    def __init__(self, config_path: str = "configs/retrieval.yaml"):
        self.logger = logging.getLogger("RetrievalOrchestrator")
        cfg_loader = ConfigLoader(config_path)
        self.cfg: Dict[str, Any] = cfg_loader.config

        opts = self.cfg.get("options", {})
        paths = self.cfg.get("paths", {})

        # Core parameters
        self.top_k = int(opts.get("top_k", 10))
        self.vector_store_dir = str(paths.get("vector_store_dir", "data/vector_store"))
        self.embedding_model = opts.get("embedding_model", "multi-qa-mpnet-base-dot-v1")
        self.diversify_sources_default = bool(opts.get("diversify_sources", True))
        oversample_factor = int(opts.get("oversample_factor", 10))
        self.max_initial = max(self.top_k * oversample_factor, self.top_k * 8)

        # Validate embedding model
        try:
            self.embed_model = SentenceTransformer(self.embedding_model)
        except Exception as e:
            raise RuntimeError(f"Failed to load embedding model '{self.embedding_model}': {e}")

        # Initialize FAISS retriever
        self.retriever = FAISSRetriever(
            vector_store_dir=self.vector_store_dir,
            model_name=self.embedding_model,
            top_k_retrieve=self.max_initial,
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_awareness=False,
            diversify_sources=self.diversify_sources_default,
        )

        # Cached reranker instance
        self._cached_reranker_type: Optional[str] = None
        self._cached_reranker = None

        self.logger.info(
            f"RetrievalOrchestrator initialized | top_k={self.top_k} | "
            f"default_diversify={self.diversify_sources_default} | model={self.embedding_model}"
        )

    # ------------------------------------------------------------------
    def retrieve(self, query: str, intent: str) -> List[Dict[str, Any]]:
        """Retrieve, rerank, and return top-k chunks based on query and detected intent."""
        if not query or not query.strip():
            self.logger.warning("Empty query ignored.")
            return []

        # Intent-dependent configuration
        is_historical = intent == "chronological"

        # Dynamically control retrieval behaviour
        self.retriever.temporal_awareness = is_historical
        self.retriever.diversify_sources = is_historical  # only enable for chronological mode

        self.logger.info(
            f"Retrieval started | intent={intent} | top_k={self.top_k} | "
            f"temporal_awareness={self.retriever.temporal_awareness} | "
            f"diversify_sources={self.retriever.diversify_sources}"
        )

        # --- initial retrieval ---
        try:
            raw_results = self.retriever.search(
                query, top_k=self.max_initial, temporal_mode=is_historical
            )
        except Exception as e:
            self.logger.exception(f"FAISS retrieval failed: {e}")
            return []

        if not raw_results:
            self.logger.warning("No retrieval results found.")
            return []

        # --- inject query text for reranker ---
        for r in raw_results:
            r["query"] = query.strip()

        # --- dynamic reranker selection ---
        reranker_type = "temporal" if is_historical else "semantic"
        if reranker_type != self._cached_reranker_type or self._cached_reranker is None:
            self._cached_reranker = RerankerFactory.from_config(
                {"options": {"reranker": reranker_type}}
            )
            self._cached_reranker_type = reranker_type

        # --- reranking phase ---
        try:
            reranked = self._cached_reranker.rerank(raw_results, top_k=len(raw_results))
        except Exception as e:
            self.logger.exception(f"Reranking failed ({reranker_type}): {e}")
            reranked = raw_results

        # --- normalization + sorting ---
        for x in reranked:
            x["final_score"] = float(x.get("final_score", x.get("score", 0.0)) or 0.0)
        reranked.sort(key=lambda x: (x["final_score"], x.get("id", "")), reverse=True)

        # --- apply diversity + relevance scoring ---
        diversified = self._enforce_diversity(reranked, self.top_k, is_historical)
        diversified = self._attach_graded_relevance(diversified, reranked)
        final = self._ensure_exact_k(diversified, self.top_k)

        # --- finalize ranks + log stats ---
        for i, x in enumerate(final, start=1):
            if not x.get("id"):
                x["id"] = make_chunk_id(x)
            x["rank"] = i

        self._log_decade_distribution(final)
        self.logger.info(
            f"Retrieval finished | returned={len(final)} | mode={intent} | "
            f"diversity={self.retriever.diversify_sources}"
        )
        return final

    # ------------------------------------------------------------------
    def _enforce_diversity(
        self, results: List[Dict[str, Any]], k: int, historical: bool
    ) -> List[Dict[str, Any]]:
        """Diversify retrieval results by source and decade (for chronological mode)."""
        if not results:
            return []

        if not historical or not self.retriever.diversify_sources:
            # Simple deduplication without diversity
            seen, out = set(), []
            for r in results:
                text = (r.get("text") or "").strip()
                if not text:
                    continue
                h = hash(text)
                if h in seen:
                    continue
                seen.add(h)
                out.append(r)
                if len(out) >= k:
                    break
            return out

        # Historical mode: enforce semantic and temporal spread
        selected, used_sources, used_decades = [], set(), set()
        pool_texts = [self._clean_text(r.get("text", "")) for r in results]
        pool_idxs = [i for i, t in enumerate(pool_texts) if t]
        if not pool_idxs:
            return results[:k]

        embs = self.embed_model.encode(
            [pool_texts[i] for i in pool_idxs], normalize_embeddings=True
        )
        kept_embs = []

        for j, idx in enumerate(pool_idxs):
            r = results[idx]
            if len(selected) >= k:
                break
            meta = r.get("metadata", {}) or {}
            src = (meta.get("source_file") or "unknown").lower()
            year = self._safe_year(r)
            decade = (year // 10) * 10 if year else None

            if src in used_sources and decade in used_decades and len(selected) < int(k * 0.8):
                continue

            cand_emb = embs[j]
            if kept_embs:
                sims = util.cos_sim(cand_emb, kept_embs)[0]
                if float(sims.max()) > 0.95:
                    continue

            selected.append(r)
            kept_embs.append(cand_emb)
            used_sources.add(src)
            if decade:
                used_decades.add(decade)

        if len(selected) < k:
            used_ids = {id(x) for x in selected}
            for r in results:
                if id(r) not in used_ids:
                    selected.append(r)
                    if len(selected) >= k:
                        break
        return selected

    # ------------------------------------------------------------------
    def _attach_graded_relevance(
        self, items: List[Dict[str, Any]], ref_population: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Assign relevance labels (0–3) based on score quantiles."""
        if not items:
            return items

        scores = np.array(
            [float(x.get("final_score", x.get("score", 0.0)) or 0.0) for x in ref_population],
            dtype=float,
        )
        if scores.size == 0 or np.allclose(scores.std(), 0.0):
            for x in items:
                x["relevance"] = 1
            return items

        try:
            q1, q2, q3 = np.quantile(scores, [0.25, 0.5, 0.75])
        except Exception:
            smin, smax = float(scores.min()), float(scores.max())
            step = (smax - smin) / 4.0 if smax > smin else 1.0
            q1, q2, q3 = smin + step, smin + 2 * step, smin + 3 * step

        for x in items:
            s = float(x.get("final_score", x.get("score", 0.0)) or 0.0)
            x["relevance"] = int(0 if s <= q1 else 1 if s <= q2 else 2 if s <= q3 else 3)
        return items

    # ------------------------------------------------------------------
    def _ensure_exact_k(self, results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        """Guarantee exactly k output results (pad if necessary)."""
        if not results:
            return []
        if len(results) > k:
            return results[:k]
        if len(results) < k:
            pad = results[-1].copy()
            results.extend(pad.copy() for _ in range(k - len(results)))
        return results

    # ------------------------------------------------------------------
    def _safe_year(self, r: Dict[str, Any]) -> Optional[int]:
        """Safely extract year from metadata."""
        meta = r.get("metadata", {}) or {}
        y = meta.get("year", r.get("year"))
        try:
            y = int(y)
            if 1900 <= y <= 2100:
                return y
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _clean_text(self, t: str) -> str:
        """Normalize whitespace for embedding stability."""
        if not t:
            return ""
        return " ".join(t.replace("\n", " ").replace("\r", " ").split())

    # ------------------------------------------------------------------
    def _log_decade_distribution(self, items: List[Dict[str, Any]]) -> None:
        """Log temporal distribution of retrieved documents."""
        hist: Dict[str, int] = defaultdict(int)
        for r in items:
            y = self._safe_year(r)
            decade = f"{(y // 10) * 10}s" if y else "unknown"
            hist[decade] += 1
        msg = ", ".join(f"{k}:{v}" for k, v in sorted(hist.items()))
        self.logger.info(f"Decade distribution: {msg}")

    # ------------------------------------------------------------------
    def close(self) -> None:
        """Gracefully close retriever resources."""
        try:
            self.retriever.close()
        except Exception as e:
            self.logger.warning(f"Failed to close retriever: {e}")
        self.logger.info("RetrievalOrchestrator closed cleanly.")

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retriever_factory.py ==== 
# src/core/retrieval/retriever_factory.py
from __future__ import annotations
import logging
from typing import Dict, Any
from src.core.retrieval.faiss_retriever import FAISSRetriever

logger = logging.getLogger(__name__)

class RetrieverFactory:
    """Factory für Intent-spezifische Retriever-Instanzen (vollständig entkoppelte Datenflüsse)."""

    @staticmethod
    def build(intent: str, cfg: Dict[str, Any]) -> FAISSRetriever:
        """Erzeuge deterministischen Retriever für den angegebenen Intent."""
        paths = cfg.get("paths", {})
        opts = cfg.get("options", {})

        # Basisparameter, deterministisch
        common_args = dict(
            model_name=opts.get("embedding_model", "all-MiniLM-L6-v2"),
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_tau=float(opts.get("temporal_tau", 8.0)),
            temporal_weight=float(opts.get("temporal_weight", 0.30)),
            top_k_retrieve=int(opts.get("top_k_retrieve", 80)),
        )

        # Intent → Index-Ordner Mapping
        index_map = {
            "conceptual": paths.get("conceptual_vector_dir", "data/vector_store/conceptual"),
            "chronological": paths.get("chronological_vector_dir", "data/vector_store/chronological"),
            "analytical": paths.get("analytical_vector_dir", "data/vector_store/analytical"),
            "comparative": paths.get("comparative_vector_dir", "data/vector_store/comparative"),
        }

        # Fallback auf globales Standardverzeichnis
        vdir = index_map.get(intent, paths.get("vector_store_dir", "data/vector_store/default"))
        temporal_flag = (intent == "chronological")

        logger.info(f"Building FAISSRetriever for intent='{intent}' | dir={vdir} | temporal={temporal_flag}")

        return FAISSRetriever(
            vector_store_dir=vdir,
            temporal_awareness=temporal_flag,
            **common_args,
        )

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\semantic_reranker.py ==== 
# src/core/retrieval/semantic_reranker.py
from __future__ import annotations
import logging
from typing import List, Dict, Any
from sentence_transformers import CrossEncoder
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class SemanticReranker(IReranker):
    """Cross-Encoder-basiertes Re-Ranking nach semantischer Relevanz."""

    def __init__(
        self,
        model_name: str,
        top_k: int = 25,
        semantic_weight: float = 0.75,
        use_gpu: bool = False,
    ):
        self.model_name = model_name
        self.top_k = top_k
        self.semantic_weight = semantic_weight
        self.device = "cuda" if use_gpu else "cpu"
        self.model = CrossEncoder(model_name, device=self.device)
        logger.info(f"Semantic Cross-Encoder loaded: {model_name} ({self.device})")

    def rerank(self, docs: List[Dict[str, Any]], top_k: int | None = None) -> List[Dict[str, Any]]:
        """Score documents via Cross-Encoder similarity."""
        if not docs:
            return []

        k = top_k or self.top_k
        pairs = [(d.get("query", ""), d.get("text", "")) for d in docs]
        scores = self.model.predict(pairs)

        for d, score in zip(docs, scores):
            base = float(d.get("score", 0.0))
            d["final_score"] = self.semantic_weight * float(score) + (1 - self.semantic_weight) * base

        docs.sort(key=lambda x: x["final_score"], reverse=True)
        return docs[:k]

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\temporal_reranker.py ==== 
from __future__ import annotations
import logging
from typing import List, Dict, Any, Tuple, Dict, Optional
from collections import defaultdict
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class TemporalReranker(IReranker):
    """
    Semantic-first temporal diversification.
    Ensures that each decade contributes at most one document,
    but only if semantic relevance is sufficiently high.
    No score manipulation. No age penalties or boosts.
    """

    def __init__(
        self,
        semantic_threshold: float = 0.40,  # minimal score required for decade coverage
        min_year: int = 1900,
        must_include: List[str] | None = None,
        blacklist_sources: List[str] | None = None
    ):
        self.semantic_threshold = semantic_threshold
        self.min_year = min_year
        self.must_include = must_include or []
        self.blacklist_sources = blacklist_sources or []

    # ------------------------------------------------------------------
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        if not results:
            return []

        # Extract valid years
        for r in results:
            r["year"] = self._extract_year(r)

        # Apply blacklists
        results = [r for r in results if not self._is_blacklisted(r)]
        if not results:
            return []

        # ---- 1. Pure semantic sort ----
        results_sorted = sorted(results, key=lambda r: r.get("score", 0.0), reverse=True)

        # ---- 2. Group by decade ----
        decade_groups = self._group_by_decade(results_sorted)

        # ---- 3. Pick top semantic document per decade (if strong) ----
        selected = []
        seen = set()

        for dec in sorted(decade_groups.keys()):
            best = decade_groups[dec][0]
            if best["score"] >= self.semantic_threshold:
                key = self._src_key(best)
                if key not in seen:
                    selected.append(best)
                    seen.add(key)

        # ---- 4. Fill remaining purely by semantic score ----
        for r in results_sorted:
            key = self._src_key(r)
            if key not in seen:
                selected.append(r)
                seen.add(key)
            if len(selected) >= top_k:
                break

        # ---- 5. Inject must-include sources at front ----
        selected = self._inject_must_include(selected, results, top_k)

        logger.info(f"Temporal diversification complete | decades={len(decade_groups)} | threshold={self.semantic_threshold}")
        return selected[:top_k]

    # ------------------------------------------------------------------
    def _extract_year(self, r):
        meta = r.get("metadata", {})
        y = meta.get("year") or r.get("year")
        try:
            y = int(str(y))
            if y < self.min_year or y > 2100:
                return self.min_year
            return y
        except Exception:
            return self.min_year

    def _group_by_decade(self, results):
        groups = defaultdict(list)
        for r in results:
            decade = (r["year"] // 10) * 10
            groups[decade].append(r)
        return groups

    def _src_key(self, r):
        meta = r.get("metadata", {})
        return (meta.get("source_file") or meta.get("title") or "unknown").lower()

    def _is_blacklisted(self, r):
        key = self._src_key(r)
        return any(b.lower() in key for b in self.blacklist_sources)

    def _inject_must_include(self, ranked, all_results, top_k):
        must = [r for r in all_results if any(m in self._src_key(r) for m in self.must_include)]
        if not must:
            return ranked
        merged, seen = [], set()
        for r in must + ranked:
            key = self._src_key(r)
            if key not in seen:
                merged.append(r)
                seen.add(key)
            if len(merged) >= top_k:
                break
        return merged

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_reranker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IReranker(ABC):
    """Interface for reranker strategies (temporal, semantic, hybrid, etc.)."""
    @abstractmethod
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        # Rerank a list of retrieval results based on a custom strategy
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_retriever.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IRetriever(ABC):
    """Interface for all retriever implementations."""
    @abstractmethod
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        # Return top-k relevant documents/chunks for a query
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\diversity_pipeline.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util

class DiversityPipeline:
    """Applies semantic and temporal diversity rules."""

    def __init__(self, embed_model: SentenceTransformer):
        self.embed_model = embed_model

    def _clean(self, t: str) -> str:
        return " ".join(t.replace("\n", " ").replace("\r", " ").split())

    def apply(self, ranked: List[Dict[str, Any]], k: int, historical: bool) -> List[Dict[str, Any]]:
        # Non-historical: simple dedupe
        if not historical:
            seen, out = set(), []
            for r in ranked:
                tx = (r.get("text") or "").strip()
                h = hash(tx)
                if h not in seen:
                    seen.add(h)
                    out.append(r)
                if len(out) >= k:
                    break
            return out

        # Historical: semantic & temporal diversity
        texts = [self._clean(r.get("text", "")) for r in ranked]
        idxs = [i for i, t in enumerate(texts) if t]
        embs = self.embed_model.encode([texts[i] for i in idxs], normalize_embeddings=True)

        selected, kept_embs = [], []
        for j, idx in enumerate(idxs):
            if len(selected) >= k:
                break
            cand = ranked[idx]
            emb = embs[j]
            if kept_embs:
                sims = util.cos_sim(emb, kept_embs)[0]
                if float(sims.max()) > 0.95:
                    continue
            selected.append(cand)
            kept_embs.append(emb)
        return selected
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\final_selector.py ==== 
from __future__ import annotations
from typing import List, Dict

class FinalSelector:
    """Ensure exact final_k chunk selection."""

    def select(self, items: List[Dict], k: int) -> List[Dict]:
        if len(items) >= k:
            return items[:k]
        if not items:
            return []
        pad = items[-1].copy()
        return items + [pad.copy() for _ in range(k - len(items))]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\relevance_annotator.py ==== 
from __future__ import annotations
from typing import List, Dict
import numpy as np

class RelevanceAnnotator:
    """Assigns 0-3 relevance labels based on score distribution."""

    def apply(self, items: List[Dict], population: List[Dict]) -> List[Dict]:
        scores = np.array([float(x.get("final_score", 0.0)) for x in population])
        if scores.size == 0 or np.allclose(scores.std(), 0.0):
            for x in items:
                x["relevance"] = 1
            return items
        q1, q2, q3 = np.quantile(scores, [0.25, 0.5, 0.75])
        for x in items:
            s = float(x.get("final_score", 0.0))
            x["relevance"] = int(0 if s <= q1 else 1 if s <= q2 else 2 if s <= q3 else 3)
        return items
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\reranking_pipeline.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
from src.core.retrieval.reranker_factory import RerankerFactory

class RerankingPipeline:
    """Unified reranking abstraction."""

    def __init__(self, cfg: Dict[str, Any]):
        self.cfg = cfg
        self.cached_type = None
        self.cached_reranker = None

    def _get(self, intent: str):
        rtype = "temporal" if intent == "chronological" else "semantic"
        if rtype != self.cached_type:
            self.cached_reranker = RerankerFactory.from_config({"options": {"reranker": rtype}})
            self.cached_type = rtype
        return self.cached_reranker

    def run(self, docs: List[Dict[str, Any]], intent: str) -> List[Dict[str, Any]]:
        reranker = self._get(intent)
        ranked = reranker.rerank(docs, top_k=len(docs))
        for d in ranked:
            d["final_score"] = float(d.get("final_score", d.get("score", 0.0)))
        ranked.sort(key=lambda x: x["final_score"], reverse=True)
        return ranked
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\retrieval_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import Dict, Any, List
from src.core.config.config_loader import ConfigLoader
from sentence_transformers import SentenceTransformer
from src.core.retrieval.faiss_retriever import FAISSRetriever

from src.core.retrieval.orchestrator.retrieval_pipeline import RetrievalPipeline
from src.core.retrieval.orchestrator.reranking_pipeline import RerankingPipeline
from src.core.retrieval.orchestrator.diversity_pipeline import DiversityPipeline
from src.core.retrieval.orchestrator.relevance_annotator import RelevanceAnnotator
from src.core.retrieval.orchestrator.final_selector import FinalSelector

from src.core.evaluation.utils import make_chunk_id


class RetrievalOrchestrator:
    """Clean multi-stage retrieval orchestrator for RAG."""

    def __init__(self, config_path: str = "configs/retrieval.yaml"):
        self.logger = logging.getLogger("RetrievalOrchestrator")
        cfg_loader = ConfigLoader(config_path)
        self.cfg = cfg_loader.config

        opts = self.cfg["options"]
        paths = self.cfg["paths"]

        # Multi-stage parameters
        self.final_k = int(opts.get("final_k", 10))
        oversample = int(opts.get("oversample_factor", 15))
        self.initial_k = max(self.final_k * oversample, self.final_k * 8)

        # Embedding model
        self.embed_model = SentenceTransformer(opts["embedding_model"])

        # Retriever instance
        self.retriever = FAISSRetriever(
            vector_store_dir=paths["vector_store_dir"],
            model_name=opts["embedding_model"],
            top_k_retrieve=self.initial_k,
            normalize_embeddings=True,
            use_gpu=opts.get("use_gpu", False),
            similarity_metric=opts.get("similarity_metric", "cosine"),
            temporal_awareness=False,
            diversify_sources=opts.get("diversify_sources", True),
        )

        # Pipeline modules
        self.stage_retrieve = RetrievalPipeline(self.retriever, self.initial_k)
        self.stage_rerank = RerankingPipeline(self.cfg)
        self.stage_diversity = DiversityPipeline(self.embed_model)
        self.stage_label = RelevanceAnnotator()
        self.stage_select = FinalSelector()

    # ------------------------------------------------------------------
    def retrieve(self, query: str, intent: str) -> List[Dict[str, Any]]:
        if not query.strip():
            return []

        historical = intent == "chronological"

        # Stage 1: Broad retrieval
        raw = self.stage_retrieve.run(query, historical)

        # Stage 2: Full reranking
        ranked = self.stage_rerank.run(raw, intent)

        # Stage 3: Diversity
        diversified = self.stage_diversity.apply(ranked, self.final_k, historical)

        # Stage 4: Relevance annotation
        annotated = self.stage_label.apply(diversified, ranked)

        # Stage 5: Final-k selection
        final = self.stage_select.select(annotated, self.final_k)

        # Assign stable IDs and ranks
        for i, x in enumerate(final, start=1):
            x["rank"] = i
            if not x.get("id"):
                x["id"] = make_chunk_id(x)

        return final

    # ------------------------------------------------------------------
    def close(self):
        self.retriever.close()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\retrieval_pipeline.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
from src.core.retrieval.faiss_retriever import FAISSRetriever

class RetrievalPipeline:
    """Handles broad FAISS retrieval only."""

    def __init__(self, retriever: FAISSRetriever, initial_k: int):
        self.retriever = retriever
        self.initial_k = initial_k

    def run(self, query: str, historical: bool) -> List[Dict[str, Any]]:
        # Perform broad retrieval without post-processing
        raw = self.retriever.search(
            query,
            top_k=self.initial_k,
            temporal_mode=historical,
        )
        for r in raw:
            r["query"] = query
        return raw
.
