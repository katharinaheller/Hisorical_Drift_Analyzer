# configs/llm.yaml
global:
  log_level: INFO

generation:
  llm:
    # Active profile for the current run (switch this for model comparison)
    profile: "mistral_7b"

  # Optional: citation and embedding settings (safe defaults if omitted)
  citations:
    style: "ieee"

  faith_embed_model: "multi-qa-mpnet-base-dot-v1"

sampling:
  # Shared sampling hyperparameters for *all* models (fair comparison)
  temperature: 0.10
  max_tokens: 2048

profiles:
  # Small model, good as lower bound
  phi3_4b:
    model: "phi3:3.8b-mini-instruct"
    auto_pull: true

  # Medium model – your default for now
  mistral_7b:
    model: "mistral:7b-instruct"
    auto_pull: true

  # Larger model – for “upper bound” comparison
  llama3_8b:
    model: "llama3:8b-instruct"
    auto_pull: true
